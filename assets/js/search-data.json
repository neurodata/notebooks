{
  
    
        "post0": {
            "title": "Generating a graph layout for the Hemibrain data",
            "content": "Load the data . Data is from the paper: . @article{xu2020connectome, title={A connectome of the adult drosophila central brain}, author={Xu, C Shan and Januszewski, Michal and Lu, Zhiyuan and Takemura, Shin-ya and Hayworth, Kenneth and Huang, Gary and Shinomiya, Kazunori and Maitin-Shepard, Jeremy and Ackerman, David and Berg, Stuart and others}, journal={BioRxiv}, year={2020}, publisher={Cold Spring Harbor Laboratory} } . import time import datetime from pathlib import Path import pandas as pd import networkx as nx from giskard.plot import graphplot from graspologic.partition import leiden from graspologic.layouts.colors import _get_colors import numpy as np from graspologic.partition import hierarchical_leiden, HierarchicalCluster from src.io import savefig from sklearn.model_selection import ParameterGrid t0 = time.time() main_random_state = np.random.default_rng(8888) def stashfig(name): savefig(name, foldername=&quot;hemibrain-layout&quot;) # TODO this is a bit buried in graspologic, should expose it more explicitly colors = _get_colors(True, None)[&quot;nominal&quot;] data_path = Path(&quot;maggot_models/data/raw/exported-traced-adjacencies-v1.1&quot;) neuron_file = &quot;traced-neurons.csv&quot; edgelist_file = &quot;traced-total-connections.csv&quot; edgelist_df = pd.read_csv(data_path / edgelist_file) g = nx.from_pandas_edgelist( edgelist_df, source=&quot;bodyId_pre&quot;, target=&quot;bodyId_post&quot;, edge_attr=&quot;weight&quot;, create_using=nx.DiGraph, ) . Run community detection for coloring the network . def symmetrze_nx(g): &quot;&quot;&quot;Leiden requires a symmetric/undirected graph. This converts a directed graph to undirected just for this community detection step&quot;&quot;&quot; sym_g = nx.Graph() for source, target, weight in g.edges.data(&quot;weight&quot;): if sym_g.has_edge(source, target): sym_g[source][target][&quot;weight&quot;] = ( sym_g[source][target][&quot;weight&quot;] + weight * 0.5 ) else: sym_g.add_edge(source, target, weight=weight * 0.5) return sym_g sym_g = symmetrze_nx(g) leiden_mode = &quot;flat&quot; if leiden_mode == &quot;hierarchical&quot;: hierarchical_partition = hierarchical_leiden( sym_g, check_directed=False, max_cluster_size=300, random_seed=int(main_random_state.integers(np.iinfo(np.int32).max)), ) partition_map = HierarchicalCluster.final_hierarchical_clustering( hierarchical_partition ) elif leiden_mode == &quot;flat&quot;: partition_map = leiden( sym_g, resolution=3, random_seed=int(main_random_state.integers(np.iinfo(np.int32).max)), ) print(f&quot;Number of unique partitions: {len(np.unique(list(partition_map.values())))}&quot;) # Define the color map nx.set_node_attributes(g, partition_map, name=&quot;partition&quot;) palette = dict(zip(np.unique(list(partition_map.values())), colors)) . Number of unique partitions: 31 . Sweep over some parameters for plotting (optional) . sweep_params = False plot_kws = dict( edge_linewidth=0.1, edge_alpha=0.1, subsample_edges=0.05, palette=palette, figsize=(12, 12), sizes=(3, 10), ) param_grid = { &quot;embedding_algorithm&quot;: [&quot;ase&quot;, &quot;lse&quot;], &quot;n_neighbors&quot;: [16, 32, 64], &quot;n_components&quot;: [8, 16, 32, 64], } def dict_to_str(dictionary): out = &quot;&quot; for key, value in dictionary.items(): out += f&quot;-{key}={value}&quot; return out if sweep_params: param_list = list(ParameterGrid(param_grid)) else: param_list = [] for embed_params in param_list: currtime = time.time() random_seed = main_random_state.integers(np.iinfo(np.int32).max) random_state = np.random.default_rng(random_seed) graphplot( g, hue=&quot;partition&quot;, verbose=1, random_state=random_state, **embed_params, **plot_kws, ) stashfig(f&quot;hemibrain-graphplot{dict_to_str(embed_params)}&quot;) print(f&quot;{time.time() - currtime:.3f} seconds elapsed to run layout.&quot;) print() . Plot my favorite layout . Running this a few times just because it&#39;s a randomized algorithm. . embed_params = dict(n_components=64, n_neighbors=64, embedding_algorithm=&quot;lse&quot;) for i in range(3): random_seed = main_random_state.integers(np.iinfo(np.int32).max) random_state = np.random.default_rng(random_seed) graphplot( g, hue=&quot;partition&quot;, verbose=1, random_state=random_state, **embed_params, **plot_kws, ) stashfig(f&quot;hemibrain-graphplot{dict_to_str(embed_params)}-seed={random_seed}&quot;) . Performing initial spectral embedding of the network... Performing UMAP embedding... Scatterplotting nodes... Collating edge data for plotting... Subsampling edges... Mapping edge data for plotting... Plotting edges... Saved figure to maggot_models/notebooks/outs/hemibrain-layout/figs/hemibrain-graphplot-n_components=64-n_neighbors=64-embedding_algorithm=lse-seed=543918345.png Performing initial spectral embedding of the network... Performing UMAP embedding... Scatterplotting nodes... Collating edge data for plotting... Subsampling edges... Mapping edge data for plotting... Plotting edges... Saved figure to maggot_models/notebooks/outs/hemibrain-layout/figs/hemibrain-graphplot-n_components=64-n_neighbors=64-embedding_algorithm=lse-seed=1404150835.png Performing initial spectral embedding of the network... Performing UMAP embedding... Scatterplotting nodes... Collating edge data for plotting... Subsampling edges... Mapping edge data for plotting... Plotting edges... Saved figure to maggot_models/notebooks/outs/hemibrain-layout/figs/hemibrain-graphplot-n_components=64-n_neighbors=64-embedding_algorithm=lse-seed=392380242.png . End . elapsed = time.time() - t0 delta = datetime.timedelta(seconds=elapsed) print(&quot;-&quot;) print(f&quot;Script took {delta}&quot;) print(f&quot;Completed at {datetime.datetime.now()}&quot;) print(&quot;-&quot;) . - Script took 0:05:49.716836 Completed at 2021-05-06 11:57:36.585274 - .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/connectome/2021/05/06/hemibrain-layout.html",
            "relUrl": "/pedigo/graspologic/connectome/2021/05/06/hemibrain-layout.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Matching ALPNs",
            "content": "Load the data . # collapse import datetime import time import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.metrics import confusion_matrix from graspologic.match import GraphMatch from graspologic.match.qap import _doubly_stochastic from src.visualization import adjplot t0 = time.time() sns.set_context(&quot;talk&quot;) meta_path = &quot;ALPN_crossmatching/data/meta.csv&quot; nblast_path = &quot;ALPN_crossmatching/data/nblast_scores.csv&quot; meta = pd.read_csv(meta_path, index_col=0) meta = meta.set_index(&quot;id&quot;) meta[&quot;label&quot;].fillna(&quot;unk&quot;, inplace=True) nblast_scores = pd.read_csv(nblast_path, index_col=0, header=0) nblast_scores.columns = nblast_scores.columns.astype(int) . . Look at the data . # collapse adjplot( nblast_scores.values, meta=meta, sort_class=[&quot;source&quot;], item_order=&quot;lineage&quot;, colors=&quot;lineage&quot;, cbar_kws=dict(shrink=0.7), ) . . (&lt;AxesSubplot:&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7f9edd9a4750&gt;, &lt;matplotlib.axes._axes.Axes at 0x7f9edd9e0410&gt;, &lt;matplotlib.axes._axes.Axes at 0x7f9edc2a02d0&gt;) . # collapse adjplot( nblast_scores.values, meta=meta, sort_class=[&quot;lineage&quot;], item_order=&quot;source&quot;, colors=&quot;source&quot;, cbar_kws=dict(shrink=0.7), ) . . (&lt;AxesSubplot:&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7f9edc65fa10&gt;, &lt;matplotlib.axes._axes.Axes at 0x7f9edc679ed0&gt;, &lt;matplotlib.axes._axes.Axes at 0x7f9edcbf1510&gt;) . Plot the distribution of pairwise scores . # collapse fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.histplot(nblast_scores.values.ravel(), element=&quot;step&quot;, stat=&quot;density&quot;) . . &lt;AxesSubplot:ylabel=&#39;Density&#39;&gt; . Split the NBLAST scores by dataset . # collapse datasets = [&quot;FAFB(L)&quot;, &quot;FAFB(R)&quot;] dataset1_meta = meta[meta[&quot;source&quot;] == datasets[0]] dataset2_meta = meta[meta[&quot;source&quot;] == datasets[1]] dataset1_ids = dataset1_meta.index dataset1_intra = nblast_scores.loc[dataset1_ids, dataset1_ids].values dataset2_ids = dataset2_meta.index dataset2_intra = nblast_scores.loc[dataset2_ids, dataset2_ids].values # TODO use these also via the linear term in GMP dataset1_to_dataset2 = nblast_scores.loc[dataset1_ids, dataset2_ids].values dataset2_to_dataset1 = nblast_scores.loc[dataset2_ids, dataset1_ids].values . . Plot the NBLAST scores before alignment . # collapse fig, axs = plt.subplots(1, 2, figsize=(12, 6)) adjplot(dataset1_intra, cbar=False, ax=axs[0]) adjplot(dataset2_intra, cbar=False, ax=axs[1]) . . (&lt;AxesSubplot:&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7f9edc973550&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;) . Run the NBLAST score matching without using any prior information . # collapse gm = GraphMatch( n_init=100, init=&quot;barycenter&quot;, max_iter=200, shuffle_input=True, eps=1e-5, gmp=True, padding=&quot;naive&quot;, ) gm.fit(dataset1_intra, dataset2_intra) perm_inds = gm.perm_inds_ print(f&quot;Matching objective function: {gm.score_}&quot;) . . Matching objective function: 6227.852163812509 . # collapse dataset2_intra_matched = dataset2_intra[perm_inds][:, perm_inds][: len(dataset1_ids)] dataset2_meta_matched = dataset2_meta.iloc[perm_inds][: len(dataset1_ids)] . . Plot the NBLAST scores after alignment . # collapse fig, axs = plt.subplots(1, 2, figsize=(12, 6)) adjplot(dataset1_intra, cbar=False, ax=axs[0]) adjplot(dataset2_intra_matched, cbar=False, ax=axs[1]) . . (&lt;AxesSubplot:&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7f9ecabedb90&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;) . Peek at the metadata after alignment . # collapse dataset1_meta . . name lineage label is_canonical ntype source . id . 12201990 neuron 12201991 mPN mALT left GD Amy | lPN | unk | False | mPN | FAFB(L) | . 12201994 neuron 12201995 mPN mALT left XZ former Dragon | lPN | unk | False | mPN | FAFB(L) | . 12202003 neuron 12202004 mPN mALT left GD Avril | lPN | unk | False | mPN | FAFB(L) | . 27 Uniglomerular mALT DM4 adPN DB ECM | adPN | DM4_adPN | True | uPN | FAFB(L) | . 2076 Uniglomerular mALT DC1 adPN PN015 DB | adPN | DC1_adPN | True | uPN | FAFB(L) | . ... ... | ... | ... | ... | ... | ... | . 2349018 Uniglomerular mALT DL3 lPN 2349019 RJVR | lPN | DL3_lPN | True | uPN | FAFB(L) | . 2349022 Uniglomerular mALT DL3 lPN 2349023 RJVR | lPN | DL3_lPN | True | uPN | FAFB(L) | . 4070 Uniglomerular mALT VC3l adPN DB XZ | adPN | VC3l_adPN | True | uPN | FAFB(L) | . 2349030 Uniglomerular mALT DL3 lPN (outlier) 2349031 RJVR | lPN | DL3_lPN | True | uPN | FAFB(L) | . 434153 Uniglomerular mALT VM1 lPN 434154 | lPN | VM1_lPN | True | uPN | FAFB(L) | . 335 rows × 6 columns . # collapse dataset2_meta_matched . . name lineage label is_canonical ntype source . id . 57430 Multiglomerular mALT lPN VP2+VL1+VP3+2 LTS 0.9... | lPN | unk | False | mPN | FAFB(R) | . 57426 Multiglomerular mALT lPN VP1d+DM4+VC1+5 LTS 0.... | lPN | unk | False | mPN | FAFB(R) | . 3648956 Multiglomerular mALT lPN VP3+VP2+VL1+1 LTS 0.9... | lPN | unk | False | mPN | FAFB(R) | . 39682 Uniglomerular mALT DM4 adPN 39683 JMR | adPN | DM4_adPN | True | uPN | FAFB(R) | . 771242 Uniglomerular mALT DC1 adPN 23665 AA | adPN | DC1_adPN | True | uPN | FAFB(R) | . ... ... | ... | ... | ... | ... | ... | . 177706 Uniglomerular mALT DL3 lPN 177707 AEB | lPN | DL3_lPN | True | uPN | FAFB(R) | . 57311 Uniglomerular mALT DA1 lPN 57312 LK | lPN | DA1_lPN | True | uPN | FAFB(R) | . 59129 Uniglomerular mALT VC3l adPN 59130 AJ | adPN | VC3l_adPN | True | uPN | FAFB(R) | . 77661 Uniglomerular mALT DL3 lPN 77662 LK | lPN | DL3_lPN | True | uPN | FAFB(R) | . 24726 Uniglomerular mALT VM1 lPN 24727 BH | lPN | VM1_lPN | True | uPN | FAFB(R) | . 335 rows × 6 columns . Plot confusion matrices for the predicted matching . # collapse def confusionplot( labels1, labels2, ax=None, figsize=(10, 10), xlabel=&quot;&quot;, ylabel=&quot;&quot;, title=&quot;Confusion matrix&quot;, annot=True, add_diag_proportion=True, **kwargs, ): unique_labels = np.unique(list(labels1) + list(labels2)) conf_mat = confusion_matrix(labels1, labels2, labels=unique_labels, normalize=None) conf_mat = pd.DataFrame(data=conf_mat, index=unique_labels, columns=unique_labels) if ax is None: _, ax = plt.subplots(1, 1, figsize=figsize) sns.heatmap( conf_mat, ax=ax, square=True, cmap=&quot;RdBu_r&quot;, center=0, cbar_kws=dict(shrink=0.6), annot=annot, fmt=&quot;d&quot;, mask=conf_mat == 0, **kwargs, ) ax.set(ylabel=ylabel, xlabel=xlabel) if add_diag_proportion: on_diag = np.trace(conf_mat.values) / np.sum(conf_mat.values) title += f&quot; ({on_diag:0.2f} correct)&quot; ax.set_title(title, fontsize=&quot;large&quot;, pad=10) return ax . . Confusion matrix for neuron type . # collapse confusionplot( dataset1_meta[&quot;ntype&quot;], dataset2_meta_matched[&quot;ntype&quot;], ylabel=datasets[0], xlabel=datasets[1], title=&quot;Type confusion matrix&quot;, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Type confusion matrix (0.96 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Confusion matrix for lineage . # collapse confusionplot( dataset1_meta[&quot;lineage&quot;], dataset2_meta_matched[&quot;lineage&quot;], ylabel=datasets[0], xlabel=datasets[1], title=&quot;Lineage confusion matrix&quot;, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Lineage confusion matrix (0.97 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Confusion matrix for label . NB: There are many &quot;unknown&quot; in the label category, which was messinig up the color palette here, so I clipped the color range at the maximum for the non-unknown categories. It could be skewing the accuracy thought (e.g. unk matched to unk counts as correct). . # collapse labels1 = dataset1_meta[&quot;label&quot;] dataset1_vmax = labels1.value_counts()[1:].max() labels2 = dataset2_meta_matched[&quot;label&quot;] dataset2_vmax = labels2.value_counts()[1:].max() vmax = max(dataset1_vmax, dataset2_vmax) confusionplot( labels1, labels2, ylabel=datasets[0], xlabel=datasets[1], title=&quot;Label confusion matrix&quot;, annot=False, vmax=vmax, xticklabels=False, yticklabels=False, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Label confusion matrix (0.90 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Accuracy for the above, ignoring unclear/unknown . # collapse unique_labels = np.unique(list(labels1) + list(labels2)) conf_mat = confusion_matrix(labels1, labels2, labels=unique_labels, normalize=None) conf_mat = pd.DataFrame(data=conf_mat, index=unique_labels, columns=unique_labels) conf_mat = conf_mat.iloc[:-5, :-5] # hack to ignore anything &quot;unclear&quot; on_diag = np.trace(conf_mat.values) / np.sum(conf_mat.values) print(f&quot;{on_diag:.2f}&quot;) . . 0.88 . Matching with a prior . Here we try to use the group label as a soft prior (not a hard constraint) on the matching proceedure. . We do this by initializing from the &quot;groupycenter&quot; as opposed to the barycenter of the doubly stochastic matrices. . Construct an initialization from the lineages . # collapse groups1 = dataset1_meta[&quot;lineage&quot;] groups2 = dataset2_meta[&quot;lineage&quot;] unique_groups = np.unique(list(groups1) + list(groups2)) n = len(groups2) # must be the size of the larger D = np.zeros((n, n)) group = unique_groups[-1] layers = [] for group in unique_groups: inds1 = np.where(groups1 == group)[0] inds2 = np.where(groups2 == group)[0] not_inds1 = np.where(groups1 != group)[0] not_inds2 = np.where(groups2 != group)[0] n_groups = [len(inds1), len(inds2)] argmax_n_group = np.argmax(n_groups) max_n_group = n_groups[argmax_n_group] if min(n_groups) != 0: val = 1 / max_n_group layer = np.zeros((n, n)) layer[np.ix_(inds1, inds2)] = val D += layer # if n_groups[0] != n_groups[1]: # if argmax_n_group == 1: # # then the column sums will be less than 0 # col_sum = layer[np.ix_(inds1, inds2)].sum(axis=0).mean() # layer[np.ix_(not_inds1, inds2)] = 1 / len(not_inds1) * (1 - col_sum) # elif argmax_n_group == 0: # # then the row sums will be less than 0 # row_sum = layer[np.ix_(inds1, inds2)].sum(axis=1).mean() # layer[np.ix_(inds1, not_inds2)] = 1 / len(not_inds2) * (1 - row_sum)_d # # # D[np.ix_(inds1, inds2)] = val # # row_sums = np.sum(layer[inds1], axis=1).mean() # # col_sums = np.sum(layer[:, inds2], axis=0).mean() # layers.append(layer) # D[:, D.sum(axis=0) == 0] = 1 / n # D[D.sum(axis=1) == 0] = 1 / n D += 1 / (n ** 2) # need to add somthing small for sinkhorn to converge D0 = _doubly_stochastic(D) . . Run matching from the informed initialization . # collapse gm = GraphMatch( n_init=100, init=D0, max_iter=200, shuffle_input=True, eps=1e-5, gmp=True, padding=&quot;naive&quot;, ) gm.fit(dataset1_intra, dataset2_intra) perm_inds = gm.perm_inds_ print(f&quot;Matching objective function: {gm.score_}&quot;) . . Matching objective function: 6229.611251407659 . # collapse dataset2_intra_matched = dataset2_intra[perm_inds][:, perm_inds][: len(dataset1_ids)] dataset2_meta_matched = dataset2_meta.iloc[perm_inds][: len(dataset1_ids)] . . Plot confusion matrices for the predicted matching started from the prior . Confusion matrix for neuron type . # collapse confusionplot( dataset1_meta[&quot;ntype&quot;], dataset2_meta_matched[&quot;ntype&quot;], ylabel=datasets[0], xlabel=datasets[1], title=&quot;Type confusion matrix&quot;, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Type confusion matrix (0.95 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Confusion matrix for lineage . # collapse confusionplot( dataset1_meta[&quot;lineage&quot;], dataset2_meta_matched[&quot;lineage&quot;], ylabel=datasets[0], xlabel=datasets[1], title=&quot;Lineage confusion matrix&quot;, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Lineage confusion matrix (0.96 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Confusion matrix for label . # collapse labels1 = dataset1_meta[&quot;label&quot;] dataset1_vmax = labels1.value_counts()[1:].max() labels2 = dataset2_meta_matched[&quot;label&quot;] dataset2_vmax = labels2.value_counts()[1:].max() vmax = max(dataset1_vmax, dataset2_vmax) confusionplot( labels1, labels2, ylabel=datasets[0], xlabel=datasets[1], title=&quot;Label confusion matrix&quot;, annot=False, vmax=vmax, xticklabels=False, yticklabels=False, ) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Label confusion matrix (0.90 correct)&#39;}, xlabel=&#39;FAFB(R)&#39;, ylabel=&#39;FAFB(L)&#39;&gt; . Observations/notes . Matching accuracy looked worse when I tried random initializations instead of barycenter | Open question of what to do with the weights themselves, I was expecting to have to use pass to ranks or some other transform but the raw scores seemed to work fairly well | &#39;VUMa2&#39; is a lineage in one FAFB and not the other hemisphere | solution using my groupycenter thing doesn&#39;t seem that different. possible that the barycenter initialization finds a similar score/matching? | . End . elapsed = time.time() - t0 delta = datetime.timedelta(seconds=elapsed) print(&quot;-&quot;) print(f&quot;Script took {delta}&quot;) print(f&quot;Completed at {datetime.datetime.now()}&quot;) print(&quot;-&quot;) . - Script took 0:11:49.256432 Completed at 2021-03-19 17:59:15.240449 - .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/graph-matching/drosophila/2021/03/19/alpn-matching.html",
            "relUrl": "/pedigo/graspologic/graph-matching/drosophila/2021/03/19/alpn-matching.html",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Kavli-ome",
            "content": "Wrangle the data . # collapse import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd import seaborn as sns from adjustText import adjust_text from giskard.plot import palplot years = np.arange(2016, 2022) savefig_kws = dict( dpi=300, pad_inches=0.3, transparent=False, bbox_inches=&quot;tight&quot;, facecolor=&quot;white&quot; ) layoutplot_kws = dict(node_alpha=0.6, adjust=True) def stashfig(name, **kwargs): plt.savefig(f&quot;sandbox/results/kavli/{name}.pdf&quot;, format=&quot;pdf&quot;, **savefig_kws) plt.savefig(f&quot;sandbox/results/kavli/{name}.png&quot;, format=&quot;png&quot;, **savefig_kws) class MapDict(dict): __missing__ = lambda self, key: key def flatten_muligraph(multigraph, filter_edge_type=None): # REF: https://stackoverflow.com/questions/15590812/networkx-convert-multigraph-into-simple-graph-with-weighted-edges if isinstance(filter_edge_type, str): filter_edge_type = [filter_edge_type] g = nx.Graph() # for node in multigraph.nodes(): # g.add_node(node) for i, j, data in multigraph.edges(data=True): edge_type = data[&quot;type&quot;] if filter_edge_type is None or edge_type in filter_edge_type: w = data[&quot;weight&quot;] if &quot;weight&quot; in data else 1.0 if g.has_edge(i, j): g[i][j][&quot;weight&quot;] += w else: g.add_edge(i, j, weight=w) return g # load the data data_loc = &quot;sandbox/data/kavli/Kavli Interconnections 2021 new.xlsx&quot; data = pd.read_excel(data_loc) data[&quot;source&quot;] = list(data[&quot;source_first&quot;] + &quot; &quot; + data[&quot;source_last&quot;]) data[&quot;target&quot;] = list(data[&quot;target_first&quot;] + &quot; &quot; + data[&quot;target_last&quot;]) # ignore anything having to do with &quot;joined&quot; for the purposes of the edgelist edges = data[data[&quot;type&quot;] != &quot;joined&quot;].copy() # these are edges that are not edges? bad_edges = edges[edges[&quot;source&quot;].isna() | edges[&quot;target&quot;].isna()] print(&quot;Ignoring bad edges:&quot;) print() bad_edges . . /Users/bpedigo/miniconda3/envs/sandbox/lib/python3.7/site-packages/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable warn(&#34;Tensorflow not installed; ParametricUMAP will be unavailable&#34;) Ignoring bad edges: . source_first source_last target_first target_last ternal target_dept start end type Unnamed: 9 ... Unnamed: 26 Unnamed: 27 Unnamed: 28 Unnamed: 29 Unnamed: 30 Unnamed: 31 Unnamed: 32 Unnamed: 33 source target . 5 Andreas | Andreou | NaN | NaN | External | Cornelia Fermuller, Shihab Shamma, Behtash Bab... | 2021.0 | 2026 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Andreas Andreou | NaN | . 6 Andreas | Andreou | NaN | NaN | External | Northrop Grumman | 2021.0 | 2025 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Andreas Andreou | NaN | . 42 Seth | Blackshaw | NaN | NaN | NaN | Neuroscience | 2020.0 | 2025 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Seth Blackshaw | NaN | . 43 Seth | Blackshaw | NaN | NaN | NaN | Neuroscience | 2020.0 | 2022 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Seth Blackshaw | NaN | . 44 Seth | Blackshaw | NaN | NaN | NaN | Neuroscience | 2021.0 | 2026 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Seth Blackshaw | NaN | . 83 Brian | Caffo | NaN | NaN | NaN | Biostat | 2020.0 | 2023 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Brian Caffo | NaN | . 123 Brian | Caffo | NaN | NaN | NaN | Biostat | NaN | 2016 | Publication | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Brian Caffo | NaN | . 125 Vikram | Chib | NaN | NaN | NaN | Neuroscience | 2019.0 | 2023 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Vikram Chib | NaN | . 126 Vikram | Chib | NaN | NaN | NaN | Neuroscience | 2019.0 | 2024 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Vikram Chib | NaN | . 302 James | Knierim | NaN | NaN | NaN | Neuroscience | 2018.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | James Knierim | NaN | . 369 Jonathan | Ling | NaN | NaN | NaN | Neuropathology | 2021.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 370 Jonathan | Ling | NaN | NaN | NaN | Neuropathology | 2021.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 371 Jonathan | Ling | NaN | NaN | NaN | Neuroscience | 2019.0 | 2020 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 378 Jonathan | Ling | NaN | NaN | NaN | Neuropathology | NaN | 2020 | Submitted Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 379 Jonathan | Ling | NaN | NaN | NaN | Neuropathology | NaN | 2020 | Submitted Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 380 Jonathan | Ling | NaN | NaN | NaN | Neuropathology | NaN | 2020 | Submitted Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Jonathan Ling | NaN | . 482 Marshall | Shuler | NaN | NaN | NaN | Neuroscience | 2020.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Marshall Shuler | NaN | . 483 Marshall | Shuler | NaN | NaN | NaN | Neuroscience | 2020.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Marshall Shuler | NaN | . 484 Marshall | Shuler | NaN | NaN | NaN | Neuroscience | 2019.0 | 2020 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Marshall Shuler | NaN | . 510 Genevieve | Stein-O&#39;Brien | NaN | NaN | External | Neuroscience | 2021.0 | Ongoing | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Genevieve Stein-O&#39;Brien | NaN | . 530 Genevieve | Stein-O&#39;Brien | NaN | NaN | External | Neuroscience | 2021.0 | NaN | Submitted Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Genevieve Stein-O&#39;Brien | NaN | . 543 Joshua | Vogelstein | NaN | NaN | Internal | BME | 2018.0 | 2020 | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Joshua Vogelstein | NaN | . 544 Joshua | Vogelstein | NaN | NaN | Internal | BME | NaN | NaN | Funded Grant | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Joshua Vogelstein | NaN | . 558 Joshua | Vogelstein | NaN | NaN | Internal | BME | 2017.0 | 2021 | Project | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Joshua Vogelstein | NaN | . 559 Joshua | Vogelstein | NaN | NaN | Internal | BME | 2017.0 | 2020 | Project | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Joshua Vogelstein | NaN | . 25 rows × 36 columns . # collapse edges = edges[~edges.index.isin(bad_edges.index)] edges = edges[edges[&quot;type&quot;] != &quot;Seminar&quot;] mg = nx.from_pandas_edgelist( edges, edge_attr=[&quot;type&quot;, &quot;ternal&quot;, &quot;start&quot;, &quot;end&quot;], create_using=nx.MultiGraph ) g = flatten_muligraph(mg) # some node metadata targets = edges[&quot;target&quot;].unique() sources = edges[&quot;source&quot;].unique() node_ids = np.unique(np.array(list(targets) + list(sources))) nodes = pd.DataFrame(index=node_ids) departments = edges.groupby(&quot;target&quot;)[&quot;target_dept&quot;].agg(lambda x: x.mode()[0]) nodes[&quot;dept&quot;] = nodes.index.map(departments).fillna(&quot;unk&quot;) edge_types = edges[&quot;type&quot;].unique() joined = data[data[&quot;type&quot;] == &quot;joined&quot;].set_index(&quot;source&quot;) nodes[&quot;core&quot;] = False nodes.loc[nodes.index.isin(joined.index), &quot;core&quot;] = True nodes[&quot;joined&quot;] = nodes.index.map(joined[&quot;start&quot;]) # mapping departments dept_map = { &quot;BME/JHM&quot;: &quot;BME&quot;, &quot;Mind Brain Institute&quot;: &quot;MBI&quot;, &quot;Mind Brain Institute, Neuroscience&quot;: &quot;Neuroscience&quot;, &quot;BME/Ophthalmology&quot;: &quot;BME&quot;, &quot;Neuro&quot;: &quot;Neuroscience&quot;, &quot;Neuro + BME&quot;: &quot;Neuroscience&quot;, &quot;Biostat&quot;: &quot;Biostats&quot;, &quot;ME&quot;: &quot;Mechanical Engineering&quot;, } dept_map = MapDict(dept_map) nodes[&quot;dept&quot;] = nodes[&quot;dept&quot;].map(dept_map) nodes.loc[&quot;Peter Searson&quot;, &quot;dept&quot;] = &quot;Materials Science and Engineering&quot; . . Create a color palette by department/institute . # collapse colors = sns.color_palette(&quot;husl&quot;, nodes[&quot;dept&quot;].nunique()) palette = dict(zip(sorted(nodes[&quot;dept&quot;].unique()), colors)) nodes[&quot;color&quot;] = nodes[&quot;dept&quot;].map(palette) sns.set_context(&quot;talk&quot;) palplot(palette) stashfig(&quot;palette&quot;) . . Plot the complete network . # collapse def layoutplot( g, pos, nodes, ax=None, figsize=(10, 10), weight_scale=1, adjust=True, log_weights=True, node_alpha=1, label_nodes=True, node_size=300, ): if ax is None: fig, ax = plt.subplots(1, 1, figsize=figsize) edgelist = g.edges() weights = np.array([g[u][v][&quot;weight&quot;] for u, v in edgelist]) # weight transformations happen here, can be important if log_weights: weights = np.log(weights + 1) weights *= weight_scale # plot the actual layout nx.draw_networkx_nodes( g, pos, nodelist=nodes.index, node_color=&quot;white&quot;, zorder=-1, node_size=node_size, ax=ax, ) nx.draw_networkx_nodes( g, pos, nodelist=nodes.index, node_color=nodes[&quot;color&quot;], alpha=node_alpha, node_size=node_size, ax=ax, ) nx.draw_networkx_edges( g, pos, edgelist=edgelist, nodelist=nodes.index, width=weights, zorder=-2, edge_color=&quot;lightgrey&quot;, alpha=1, ax=ax, node_size=node_size, # connectionstyle=&quot;arc3,rad=0.2&quot; ) if label_nodes: texts = [] for node in nodes.index: node_pos = pos[node] split = node.split(&quot; &quot;) name = split[0] + &quot; n&quot; name += node[len(split[0]) + 1 :] text = ax.text( node_pos[0], node_pos[1], name, ha=&quot;center&quot;, va=&quot;center&quot;, fontname=&quot;DejaVu Sans&quot;, ) # text.set_bbox(dict(facecolor=&quot;white&quot;, alpha=0.3, edgecolor=&quot;white&quot;)) texts.append(text) if adjust: adjust_text( texts, expand_text=(1.03, 1.1), lim=200, avoid_self=False, autoalign=False, ) ax.axis(&quot;off&quot;) return ax sns.set_context(&quot;talk&quot;, font_scale=0.55) fig, ax = plt.subplots(1, 1, figsize=(12, 12)) pos = nx.kamada_kawai_layout(g, weight=None) layoutplot(g, pos, nodes, ax=ax, **layoutplot_kws) stashfig(&quot;kavli-all&quot;) . . Plot the core Kavli group by year . # collapse def calculate_bounds(pos, pad=0.05): xy = np.stack(list(pos.values()), axis=1).T xmin = xy[:, 0].min() xmax = xy[:, 0].max() ymin = xy[:, 1].min() ymax = xy[:, 1].max() xpad = (xmax - xmin) * pad ypad = (ymax - ymin) * pad xlim = (xmin - xpad, xmax + xpad) ylim = (ymin - ypad, ymax + ypad) return xlim, ylim def extract_year_mgs(mg): year_mgs = {} for year in years: select_edges = [] for (u, v, k, d) in mg.edges(data=True, keys=True): edge_year = d[&quot;start&quot;] if edge_year &lt;= year: select_edges.append((u, v, k)) year_mg = nx.edge_subgraph(mg, select_edges) year_mgs[year] = year_mg return year_mgs # layoutplot_kws[&quot;adjust&quot;] = True core_mg = nx.subgraph(mg, nodes[nodes[&quot;core&quot;]].index) core_flat_g = flatten_muligraph(core_mg) core_pos = nx.kamada_kawai_layout(core_flat_g, weight=None) xlim, ylim = calculate_bounds(core_pos) year_mgs = extract_year_mgs(core_mg) for year in years: year_mg = year_mgs[year] year_g = flatten_muligraph(year_mg) fig, ax = plt.subplots(1, 1, figsize=(12, 12)) layoutplot( year_g, core_pos, nodes[nodes.index.isin(core_flat_g.nodes)], ax=ax, **layoutplot_kws, ) ax.set_title(year, fontsize=30) ax.set(xlim=xlim, ylim=ylim) stashfig(f&quot;kavli-core-year-{year}&quot;) . . Plot the entire network by year . # collapse flat_g = flatten_muligraph(mg) pos = nx.kamada_kawai_layout(flat_g, weight=None) xlim, ylim = calculate_bounds(pos) year_mgs = extract_year_mgs(mg) for year in years: year_mg = year_mgs[year] year_g = flatten_muligraph(year_mg) fig, ax = plt.subplots(1, 1, figsize=(12, 12)) layoutplot( year_g, pos, nodes[nodes.index.isin(year_g.nodes)], ax=ax, **layoutplot_kws ) ax.set_title(year, fontsize=30) ax.set(xlim=xlim, ylim=ylim) stashfig(f&quot;kavli-all-year-{year}&quot;) . . Plot each type of connection separately . # collapse for edge_type in edge_types: edge_type_g = flatten_muligraph(mg, filter_edge_type=edge_type) fig, ax = plt.subplots(1, 1, figsize=(12, 12)) layoutplot( edge_type_g, pos, nodes[nodes.index.isin(edge_type_g.nodes)], ax=ax, **layoutplot_kws, ) ax.set_title(edge_type, fontsize=30) ax.set(xlim=xlim, ylim=ylim) stashfig(f&quot;kavli-all-edge-type-{edge_type}&quot;) . . Plot each type of connection on the same set of axes . fig, axs = plt.subplots(2, 4, figsize=(20, 10)) for i, edge_type in enumerate(edge_types): ax = axs.flat[i] edge_type_g = flatten_muligraph(mg, filter_edge_type=edge_type) layoutplot( edge_type_g, pos, nodes[nodes.index.isin(edge_type_g.nodes)], ax=ax, node_size=100, label_nodes=False, **layoutplot_kws, ) ax.set_title(edge_type, fontsize=20) ax.set(xlim=xlim, ylim=ylim) for ax in axs.flat: ax.axis(&quot;off&quot;) stashfig(f&quot;kavli-all-edge-types&quot;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/plotting/2021/03/18/kavli.html",
            "relUrl": "/pedigo/graspologic/plotting/2021/03/18/kavli.html",
            "date": " • Mar 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Ranking teams by minimizing upsets via graph matching",
            "content": "Constructing the graph . Here I construct a network based on the games played during the 2020 NCAA football season. The nodes in the network represent teams and the directed edges represent a victory when team $i$ played team $j$. . import re from pathlib import Path import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from graspologic.match import GraphMatch from graspologic.plot import adjplot sns.set_context(&quot;talk&quot;) savefig_kws = dict( dpi=300, pad_inches=0.3, transparent=False, bbox_inches=&quot;tight&quot;, facecolor=&quot;white&quot; ) output_path = Path(&quot;sandbox/results/upset_ranking/&quot;) # REF: https://www.sports-reference.com/cfb/years/2020.html schedule = pd.read_csv(&quot;sandbox/data/ncaa_football_2020.csv&quot;) def filt(string): return re.sub(r&quot; ([0123456789]* ) &quot;, &quot;&quot;, string) vec_filt = np.vectorize(filt) schedule[&quot;Winner&quot;] = vec_filt(schedule[&quot;Winner&quot;]) schedule[&quot;Loser&quot;] = vec_filt(schedule[&quot;Loser&quot;]) unique_winners = np.unique(schedule[&quot;Winner&quot;]) unique_losers = np.unique(schedule[&quot;Loser&quot;]) teams = np.union1d(unique_winners, unique_losers) n_teams = len(teams) adjacency_df = pd.DataFrame( index=teams, columns=teams, data=np.zeros((n_teams, n_teams)) ) for idx, row in schedule.iterrows(): winner = row[&quot;Winner&quot;] loser = row[&quot;Loser&quot;] adjacency_df.loc[winner, loser] += 1 remove_winless = False n_wins = adjacency_df.sum(axis=1) if remove_winless: teams = teams[n_wins &gt; 0] n_wins = adjacency_df.sum(axis=1) adjacency_df = adjacency_df.reindex(index=teams, columns=teams) n_teams = len(teams) ax, _ = adjplot(adjacency_df.values, plot_type=&quot;scattermap&quot;, sizes=(10, 10), marker=&quot;s&quot;) ax.set_title(&quot;2020 NCAA Footbal Season Graph&quot;, fontsize=25) ax.set(ylabel=&quot;Winning team&quot;) ax.set(xlabel=&quot;Losing team&quot;) plt.savefig(output_path / &quot;unsorted_adjacency.png&quot;, **savefig_kws) print(f&quot;Number of teams {n_teams}&quot;) . /Users/bpedigo/miniconda3/envs/sandbox/lib/python3.7/site-packages/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable warn(&#34;Tensorflow not installed; ParametricUMAP will be unavailable&#34;) Number of teams 143 . Matching to a flat upper triangular matrix . Under a given sorting (permutation) of the adjacency matrix, any game (edge) that is an upset will fall in the lower triangle, because a lower-ranked team beat a higher-ranked team. We can therefore create a ranking by graph matching the adjacency matrix to a flat upper triangular matrix, thereby inducing a sorting/ranking that minimizes the number of upsets. . adj = adjacency_df.values # constructing the match matrix match_mat = np.zeros_like(adj) triu_inds = np.triu_indices(len(match_mat), k=1) match_mat[triu_inds] = 1 # running graph matching np.random.seed(8888) gm = GraphMatch(n_init=500, max_iter=150, eps=1e-6) gm.fit(match_mat, adj) perm_inds = gm.perm_inds_ adj_matched = adj[perm_inds][:, perm_inds] upsets = adj_matched[triu_inds[::-1]].sum() n_games = adj_matched.sum() print(f&quot;Number of games: {n_games}&quot;) print(f&quot;Number of non-upsets (graph matching score): {gm.score_}&quot;) print(f&quot;Number of upsets: {upsets}&quot;) print(f&quot;Upset ratio: {upsets/n_games}&quot;) print() print(&quot;Ranking:&quot;) print(teams[perm_inds]) . Number of games: 569.0 Number of non-upsets (graph matching score): 508.0 Number of upsets: 61.0 Upset ratio: 0.10720562390158173 Ranking: [&#39;Coastal Carolina&#39; &#39;Oklahoma&#39; &#39;Alabama&#39; &#39;Georgia&#39; &#39;Ohio State&#39; &#39;Cincinnati&#39; &#39;Texas A&amp;M&#39; &#39;Notre Dame&#39; &#39;Ball State&#39; &#39;North Carolina&#39; &#39;Louisiana&#39; &#39;Florida&#39; &#39;Iowa State&#39; &#39;Clemson&#39; &#39;Texas Christian&#39; &#39;Appalachian State&#39; &#39;Texas&#39; &#39;San Jose State&#39; &#39;Western Michigan&#39; &#39;Buffalo&#39; &#39;Oklahoma State&#39; &#39;Miami (FL)&#39; &#39;Tulsa&#39; &#39;Oregon&#39; &#39;Northwestern&#39; &#39;Iowa&#39; &#39;Georgia State&#39; &#39;Southern Methodist&#39; &#39;Texas Tech&#39; &#39;West Virginia&#39; &#39;Boise State&#39; &#39;North Carolina State&#39; &#39;Miami (OH)&#39; &#39;Colorado&#39; &#39;Alabama-Birmingham&#39; &#39;Kent State&#39; &#39;Memphis&#39; &#39;Central Florida&#39; &#39;Marshall&#39; &#39;Toledo&#39; &#39;Army&#39; &#39;Stanford&#39; &#39;San Diego State&#39; &#39;Auburn&#39; &#39;Air Force&#39; &#39;Pittsburgh&#39; &#39;Hawaii&#39; &#39;Georgia Southern&#39; &#39;Troy&#39; &#39;Liberty&#39; &#39;Mercer&#39; &#39;Florida Atlantic&#39; &#39;Texas-San Antonio&#39; &#39;Nevada&#39; &#39;Washington&#39; &#39;Southern California&#39; &#39;Houston&#39; &#39;UCLA&#39; &#39;Mississippi&#39; &#39;Indiana&#39; &#39;Wisconsin&#39; &#39;New Mexico&#39; &#39;Tarleton State&#39; &#39;Wake Forest&#39; &#39;Missouri&#39; &#39;Maryland&#39; &#39;Louisiana Tech&#39; &#39;Central Michigan&#39; &#39;Penn State&#39; &#39;Utah&#39; &#39;Louisiana State&#39; &#39;Massachusetts&#39; &#39;Virginia Tech&#39; &#39;Western Carolina&#39; &#39;Georgia Tech&#39; &#39;Virginia&#39; &#39;Washington State&#39; &#39;Michigan&#39; &#39;Campbell&#39; &#39;Boston College&#39; &#39;Louisville&#39; &#39;Fresno State&#39; &#39;Western Kentucky&#39; &#39;Minnesota&#39; &#39;Charlotte&#39; &#39;Illinois&#39; &#39;Eastern Kentucky&#39; &#39;Colorado State&#39; &#39;Austin Peay&#39; &#39;Arizona State&#39; &#39;Utah State&#39; &#39;Navy&#39; &#39;Tulane&#39; &#39;South Alabama&#39; &#39;Southern Mississippi&#39; &#39;Nebraska&#39; &#39;Eastern Michigan&#39; &#39;Kentucky&#39; &#39;Northern Illinois&#39; &#39;Ohio&#39; &#39;North Texas&#39; &#39;Texas-El Paso&#39; &#39;East Carolina&#39; &#39;Middle Tennessee State&#39; &#39;Rutgers&#39; &#39;New Mexico State&#39; &#39;Arizona&#39; &#39;Abilene Christian&#39; &#39;Stephen F. Austin&#39; &#39;Arkansas&#39; &#39;Purdue&#39; &#39;Baylor&#39; &#39;Oregon State&#39; &#39;Texas State&#39; &#39;Chattanooga&#39; &#39;Arkansas State&#39; &#39;Houston Baptist&#39; &#39;Rice&#39; &#39;Tennessee&#39; &#39;Wyoming&#39; &#39;California&#39; &#39;Michigan State&#39; &#39;Akron&#39; &#39;Central Arkansas&#39; &#39;Florida State&#39; &#39;Bowling Green State&#39; &#39;Temple&#39; &#39;Jacksonville State&#39; &#39;Mississippi State&#39; &#39;North Alabama&#39; &#39;Florida International&#39; &#39;Duke&#39; &#39;Nevada-Las Vegas&#39; &#39;South Carolina&#39; &#39;Vanderbilt&#39; &#39;South Florida&#39; &#39;Kansas State&#39; &#39;Brigham Young&#39; &#39;Citadel&#39; &#39;Syracuse&#39; &#39;Louisiana-Monroe&#39; &#39;Missouri State&#39; &#39;Kansas&#39;] . Plotting the matched (ranked) graph . ax, _ = adjplot(adj_matched, plot_type=&quot;scattermap&quot;, sizes=(10, 10), marker=&quot;s&quot;) ax.plot([0, n_teams], [0, n_teams], linewidth=1, color=&quot;black&quot;, linestyle=&quot;-&quot;) ylabel = r&quot;$ leftarrow$ Ranked low &quot; ylabel += &quot;Winning team &quot; ylabel += r&quot;Ranked high $ rightarrow$&quot; ax.set_ylabel(ylabel, fontsize=&quot;large&quot;) ax.set(xlabel=&quot;Losing team&quot;) ax.set_title(&quot;2020 NCAA Footbal Season Graph&quot;, fontsize=25) ax.fill_between( [0, n_teams], [0, n_teams], [n_teams, n_teams], zorder=-1, alpha=0.4, color=&quot;lightgrey&quot;, ) ax.text(n_teams / 4, 3 / 4 * n_teams, &quot;Upsets&quot;) plt.savefig(output_path / &quot;ranked_adjacency.png&quot;, **savefig_kws) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/graph-match/2021/03/05/ranking-via-gm.html",
            "relUrl": "/pedigo/graspologic/graph-match/2021/03/05/ranking-via-gm.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Graph matching with "Light Speed Sinkhorn Distances"",
            "content": "Experiment Summary, n = 150 . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0.5,0.6, ldots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{LS}$ (where $GM_{LS}$ uses the &quot;Lightspeed sinkhorn distances&quot; for computing the step direction) each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$ . . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . Below contains figures for $r=50$, $t=10$ . #collapse from scipy.stats import sem error = [sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;poster&#39;) sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+LS&#39;, color=&#39;blue&#39;) # plt.errorbar(rhos,average_gmlap, error_gmlap,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+dissim(dist)&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) # plt.errorbar(rhos,average_lap, error_lap,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;LAP (dist)&#39;, color=&#39;yellow&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.title(&#39;n=150&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fe0343bc1d0&gt; . Experiment Summary, n = 1500 . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0.8, 0.85, 0.9, 0.95, 1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{LS}$ (where $GM_{LS}$ uses the &quot;Lightspeed sinkhorn distances&quot; for computing the step direction) each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$ . . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ s.e. . Below contains figures for $r=25$, $t=5$ . #collapse from scipy.stats import sem # error_1500 = [sem(ratios[i,:]) for i in range(n_p)] # average_1500 = [np.mean(ratios[i,:] ) for i in range(n_p)] # error_ss_1500 = [sem(ratios_ss[i,:]) for i in range(n_p)] # average_ss_1500 = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;poster&#39;) sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.figure() plt.errorbar(rhos_1500,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+LS&#39;, color=&#39;blue&#39;) # plt.errorbar(rhos,average_gmlap, error_gmlap,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+dissim(dist)&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) # plt.errorbar(rhos,average_lap, error_lap,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;LAP (dist)&#39;, color=&#39;yellow&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) # plt.text(0.5,0.5,txt) plt.title(f&#39;n = 1500, {txt}&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa0748175c0&gt; . Vanilla GM . #collapse import numpy as np import operator from scipy.optimize import linear_sum_assignment, OptimizeResult from scipy._lib._util import check_random_state import itertools def quadratic_assignment(A, B, method=&quot;faq&quot;, options=None): r&quot;&quot;&quot; Approximates solution to the quadratic assignment problem and the graph matching problem. Quadratic assignment solves problems of the following form: .. math:: min_P &amp; { text{trace}(A^T P B P^T)} mbox{s.t. } &amp; {P epsilon mathcal{P}} where :math:` mathcal{P}` is the set of all permutation matrices, and :math:`A` and :math:`B` are square matrices. Graph matching tries to *maximize* the same objective function. This algorithm can be thought of as finding the alignment of the nodes of two graphs that minimizes the number of induced edge disagreements, or, in the case of weighted graphs, the sum of squared edge weight differences. Note that the quadratic assignment problem is NP-hard, is not known to be solvable in polynomial time, and is computationally intractable. Therefore, the results given are approximations, not guaranteed to be exact solutions. Parameters - A : 2d-array, square The square matrix :math:`A` in the objective function above. B : 2d-array, square The square matrix :math:`B` in the objective function above. method : str in {&#39;faq&#39;, &#39;2opt&#39;} (default: &#39;faq&#39;) The algorithm used to solve the problem. :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;` (default) and :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` are available. options : dict, optional A dictionary of solver options. All solvers support the following: partial_match : 2d-array of integers, optional, (default = None) Allows the user to fix part of the matching between the two matrices. In the literature, a partial match is also known as a &quot;seed&quot; [2]_. Each row of `partial_match` specifies the indices of a pair of corresponding nodes, that is, node ``partial_match[i, 0]`` of `A` is matched to node ``partial_match[i, 1]`` of `B`. Accordingly, ``partial_match`` is an array of size ``(m , 2)``, where ``m`` is not greater than the number of nodes. maximize : bool (default = False) Setting `maximize` to ``True`` solves the Graph Matching Problem (GMP) rather than the Quadratic Assingnment Problem (QAP). rng : {None, int, `~np.random.RandomState`, `~np.random.Generator`} This parameter defines the object to use for drawing random variates. If `rng` is ``None`` the `~np.random.RandomState` singleton is used. If `rng` is an int, a new ``RandomState`` instance is used, seeded with `rng`. If `rng` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. For method-specific options, see :func:`show_options(&#39;quadratic_assignment&#39;) &lt;show_options&gt;`. Returns - res : OptimizeResult A :class:`scipy.optimize.OptimizeResult` containing the following fields. col_ind : 1-D array An array of column indices corresponding with the best permutation of the nodes of `B` found. fun : float The corresponding value of the objective function. nit : int The number of iterations performed during optimization. Notes -- The default method :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;` uses the Fast Approximate QAP algorithm [1]_; it is typically offers the best combination of speed and accuracy. Method :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` can be computationally expensive, but may be a useful alternative, or it can be used to refine the solution returned by another method. References - .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik, S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and C.E. Priebe, &quot;Fast approximate quadratic programming for graph matching,&quot; PLOS one, vol. 10, no. 4, p. e0121002, 2015, :doi:`10.1371/journal.pone.0121002` .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski, C. Priebe, &quot;Seeded graph matching&quot;, Pattern Recognit. 87 (2019): 203-215, :doi:`10.1016/j.patcog.2018.09.014` .. [3] &quot;2-opt,&quot; Wikipedia. https://en.wikipedia.org/wiki/2-opt Examples -- &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; from scipy.optimize import quadratic_assignment &gt;&gt;&gt; A = np.array([[0, 80, 150, 170], [80, 0, 130, 100], ... [150, 130, 0, 120], [170, 100, 120, 0]]) &gt;&gt;&gt; B = np.array([[0, 5, 2, 7], [0, 0, 3, 8], ... [0, 0, 0, 3], [0, 0, 0, 0]]) &gt;&gt;&gt; res = quadratic_assignment(A, B) &gt;&gt;&gt; print(res) col_ind: array([0, 3, 2, 1]) fun: 3260 nit: 9 The see the relationship between the returned ``col_ind`` and ``fun``, use ``col_ind`` to form the best permutation matrix found, then evaluate the objective function :math:`f(P) = trace(A^T P B P^T )`. &gt;&gt;&gt; n = A.shape[0] &gt;&gt;&gt; perm = res[&#39;col_ind&#39;] &gt;&gt;&gt; P = np.eye(n, dtype=int)[perm] &gt;&gt;&gt; fun = np.trace(A.T @ P @ B @ P.T) &gt;&gt;&gt; print(fun) 3260 Alternatively, to avoid constructing the permutation matrix explicitly, directly permute the rows and columns of the distance matrix. &gt;&gt;&gt; fun = np.trace(A.T @ B[perm][:, perm]) &gt;&gt;&gt; print(fun) 3260 Although not guaranteed in general, ``quadratic_assignment`` happens to have found the globally optimal solution. &gt;&gt;&gt; from itertools import permutations &gt;&gt;&gt; perm_opt, fun_opt = None, np.inf &gt;&gt;&gt; for perm in permutations([0, 1, 2, 3]): ... perm = np.array(perm) ... fun = np.trace(A.T @ B[perm][:, perm]) ... if fun &lt; fun_opt: ... fun_opt, perm_opt = fun, perm &gt;&gt;&gt; print(np.array_equal(perm_opt, res[&#39;col_ind&#39;])) True Here is an example for which the default method, :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;`, does not find the global optimum. &gt;&gt;&gt; A = np.array([[0, 5, 8, 6], [5, 0, 5, 1], ... [8, 5, 0, 2], [6, 1, 2, 0]]) &gt;&gt;&gt; B = np.array([[0, 1, 8, 4], [1, 0, 5, 2], ... [8, 5, 0, 5], [4, 2, 5, 0]]) &gt;&gt;&gt; res = quadratic_assignment(A, B) &gt;&gt;&gt; print(res) col_ind: array([1, 0, 3, 2]) fun: 178 nit: 13 If accuracy is important, consider using :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` to refine the solution. &gt;&gt;&gt; guess = np.array([np.arange(A.shape[0]), res.col_ind]).T &gt;&gt;&gt; res = quadratic_assignment(A, B, method=&quot;2opt&quot;, ... options = {&#39;partial_guess&#39;: guess}) &gt;&gt;&gt; print(res) col_ind: array([1, 2, 3, 0]) fun: 176 nit: 17 &quot;&quot;&quot; if options is None: options = {} method = method.lower() methods = {&quot;faq&quot;: _quadratic_assignment_faq} if method not in methods: raise ValueError(f&quot;method {method} must be in {methods}.&quot;) res = methods[method](A, B, **options) return res def _calc_score(A, B, perm): # equivalent to objective function but avoids matmul return np.sum(A * B[perm][:, perm]) def _common_input_validation(A, B, partial_match): A = np.atleast_2d(A) B = np.atleast_2d(B) if partial_match is None: partial_match = np.array([[], []]).T partial_match = np.atleast_2d(partial_match).astype(int) msg = None if A.shape[0] != A.shape[1]: msg = &quot;`A` must be square&quot; elif B.shape[0] != B.shape[1]: msg = &quot;`B` must be square&quot; elif A.ndim != 2 or B.ndim != 2: msg = &quot;`A` and `B` must have exactly two dimensions&quot; elif A.shape != B.shape: msg = &quot;`A` and `B` matrices must be of equal size&quot; elif partial_match.shape[0] &gt; A.shape[0]: msg = &quot;`partial_match` can have only as many seeds as there are nodes&quot; elif partial_match.shape[1] != 2: msg = &quot;`partial_match` must have two columns&quot; elif partial_match.ndim != 2: msg = &quot;`partial_match` must have exactly two dimensions&quot; elif (partial_match &lt; 0).any(): msg = &quot;`partial_match` must contain only positive indices&quot; elif (partial_match &gt;= len(A)).any(): msg = &quot;`partial_match` entries must be less than number of nodes&quot; elif not len(set(partial_match[:, 0])) == len(partial_match[:, 0]) or not len( set(partial_match[:, 1]) ) == len(partial_match[:, 1]): msg = &quot;`partial_match` column entries must be unique&quot; if msg is not None: raise ValueError(msg) return A, B, partial_match def _quadratic_assignment_faq( A, B, maximize=False, partial_match=None, rng=None, P0=&quot;barycenter&quot;, shuffle_input=False, maxiter=30, tol=0.03, ): r&quot;&quot;&quot; Solve the quadratic assignment problem (approximately). This function solves the Quadratic Assignment Problem (QAP) and the Graph Matching Problem (GMP) using the Fast Approximate QAP Algorithm (FAQ) [1]_. Quadratic assignment solves problems of the following form: .. math:: min_P &amp; { text{trace}(A^T P B P^T)} mbox{s.t. } &amp; {P epsilon mathcal{P}} where :math:` mathcal{P}` is the set of all permutation matrices, and :math:`A` and :math:`B` are square matrices. Graph matching tries to *maximize* the same objective function. This algorithm can be thought of as finding the alignment of the nodes of two graphs that minimizes the number of induced edge disagreements, or, in the case of weighted graphs, the sum of squared edge weight differences. Note that the quadratic assignment problem is NP-hard, is not known to be solvable in polynomial time, and is computationally intractable. Therefore, the results given are approximations, not guaranteed to be exact solutions. Parameters - A : 2d-array, square The square matrix :math:`A` in the objective function above. B : 2d-array, square The square matrix :math:`B` in the objective function above. method : str in {&#39;faq&#39;, &#39;2opt&#39;} (default: &#39;faq&#39;) The algorithm used to solve the problem. This is the method-specific documentation for &#39;faq&#39;. :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` is also available. Options - maximize : bool (default = False) Setting `maximize` to ``True`` solves the Graph Matching Problem (GMP) rather than the Quadratic Assingnment Problem (QAP). This is accomplished through trivial negation of the objective function. rng : {None, int, `~np.random.RandomState`, `~np.random.Generator`} This parameter defines the object to use for drawing random variates. If `rng` is ``None`` the `~np.random.RandomState` singleton is used. If `rng` is an int, a new ``RandomState`` instance is used, seeded with `rng`. If `rng` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. partial_match : 2d-array of integers, optional, (default = None) Allows the user to fix part of the matching between the two matrices. In the literature, a partial match is also known as a &quot;seed&quot;. Each row of `partial_match` specifies the indices of a pair of corresponding nodes, that is, node ``partial_match[i, 0]`` of `A` is matched to node ``partial_match[i, 1]`` of `B`. Accordingly, ``partial_match`` is an array of size ``(m , 2)``, where ``m`` is not greater than the number of nodes, :math:`n`. P0 : 2d-array, &quot;barycenter&quot;, or &quot;randomized&quot; (default = &quot;barycenter&quot;) The initial (guess) permutation matrix or search &quot;position&quot; `P0`. `P0` need not be a proper permutation matrix; however, it must be :math:`m&#39; x m&#39;`, where :math:`m&#39; = n - m`, and it must be doubly stochastic: each of its rows and columns must sum to 1. If unspecified or ``&quot;barycenter&quot;``, the non-informative &quot;flat doubly stochastic matrix&quot; :math:`J = 1*1^T/m&#39;`, where :math:`1` is a :math:`m&#39; times 1` array of ones, is used. This is the &quot;barycenter&quot; of the search space of doubly-stochastic matrices. If ``&quot;randomized&quot;``, the algorithm will start from the randomized initial search position :math:`P_0 = (J + K)/2`, where :math:`J` is the &quot;barycenter&quot; and :math:`K` is a random doubly stochastic matrix. shuffle_input : bool (default = False) To avoid artificially high or low matching due to inherent sorting of input matrices, gives users the option to shuffle the nodes. Results are then unshuffled so that the returned results correspond with the node order of inputs. Shuffling may cause the algorithm to be non-deterministic, unless a random seed is set or an `rng` option is provided. maxiter : int, positive (default = 30) Integer specifying the max number of Franke-Wolfe iterations performed. tol : float (default = 0.03) A threshold for the stopping criterion. Franke-Wolfe iteration terminates when the change in search position between iterations is sufficiently small, that is, when the relative Frobenius norm, :math:` frac{||P_{i}-P_{i+1}||_F}{ sqrt{len(P_{i})}} leq tol`, where :math:`i` is the iteration number. Returns - res : OptimizeResult A :class:`scipy.optimize.OptimizeResult` containing the following fields. col_ind : 1-D array An array of column indices corresponding with the best permutation of the nodes of `B` found. fun : float The corresponding value of the objective function. nit : int The number of Franke-Wolfe iterations performed. Notes -- The algorithm may be sensitive to the initial permutation matrix (or search &quot;position&quot;) due to the possibility of several local minima within the feasible region. A barycenter initialization is more likely to result in a better solution than a single random initialization. However, ``quadratic_assignment`` calling several times with different random initializations may result in a better optimum at the cost of longer total execution time. Examples -- As mentioned above, a barycenter initialization often results in a better solution than a single random initialization. &gt;&gt;&gt; np.random.seed(0) &gt;&gt;&gt; n = 15 &gt;&gt;&gt; A = np.random.rand(n, n) &gt;&gt;&gt; B = np.random.rand(n, n) &gt;&gt;&gt; res = quadratic_assignment(A, B) # FAQ is default method &gt;&gt;&gt; print(res.fun) 46.871483385480545 # may vary &gt;&gt;&gt; options = {&quot;P0&quot;: &quot;randomized&quot;} # use randomized initialization &gt;&gt;&gt; res = quadratic_assignment(A, B, options=options) &gt;&gt;&gt; print(res.fun) 47.224831071310625 # may vary However, consider running from several randomized initializations and keeping the best result. &gt;&gt;&gt; res = min([quadratic_assignment(A, B, options=options) ... for i in range(30)], key=lambda x: x.fun) &gt;&gt;&gt; print(res.fun) 46.671852533681516 # may vary The &#39;2-opt&#39; method can be used to further refine the results. &gt;&gt;&gt; options = {&quot;partial_guess&quot;: np.array([np.arange(n), res.col_ind]).T} &gt;&gt;&gt; res = quadratic_assignment(A, B, method=&quot;2opt&quot;, options=options) &gt;&gt;&gt; print(res.fun) 46.47160735721583 # may vary References - .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik, S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and C.E. Priebe, &quot;Fast approximate quadratic programming for graph matching,&quot; PLOS one, vol. 10, no. 4, p. e0121002, 2015, :doi:`10.1371/journal.pone.0121002` .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski, C. Priebe, &quot;Seeded graph matching&quot;, Pattern Recognit. 87 (2019): 203-215, :doi:`10.1016/j.patcog.2018.09.014` &quot;&quot;&quot; maxiter = operator.index(maxiter) # ValueError check A, B, partial_match = _common_input_validation(A, B, partial_match) msg = None if isinstance(P0, str) and P0 not in {&quot;barycenter&quot;, &quot;randomized&quot;}: msg = &quot;Invalid &#39;P0&#39; parameter string&quot; elif maxiter &lt;= 0: msg = &quot;&#39;maxiter&#39; must be a positive integer&quot; elif tol &lt;= 0: msg = &quot;&#39;tol&#39; must be a positive float&quot; if msg is not None: raise ValueError(msg) rng = check_random_state(rng) n = A.shape[0] # number of vertices in graphs n_seeds = partial_match.shape[0] # number of seeds n_unseed = n - n_seeds # check outlier cases if n == 0 or partial_match.shape[0] == n: score = _calc_score(A, B, partial_match[:, 1]) res = {&quot;col_ind&quot;: partial_match[:, 1], &quot;fun&quot;: score, &quot;nit&quot;: 0} return OptimizeResult(res) obj_func_scalar = 1 if maximize: obj_func_scalar = -1 nonseed_B = np.setdiff1d(range(n), partial_match[:, 1]) if shuffle_input: nonseed_B = rng.permutation(nonseed_B) # shuffle_input to avoid results from inputs that were already matched nonseed_A = np.setdiff1d(range(n), partial_match[:, 0]) perm_A = np.concatenate([partial_match[:, 0], nonseed_A]) perm_B = np.concatenate([partial_match[:, 1], nonseed_B]) # definitions according to Seeded Graph Matching [2]. A11, A12, A21, A22 = _split_matrix(A[perm_A][:, perm_A], n_seeds) B11, B12, B21, B22 = _split_matrix(B[perm_B][:, perm_B], n_seeds) # [1] Algorithm 1 Line 1 - choose initialization if isinstance(P0, str): # initialize J, a doubly stochastic barycenter J = np.ones((n_unseed, n_unseed)) / n_unseed if P0 == &quot;barycenter&quot;: P = J elif P0 == &quot;randomized&quot;: # generate a nxn matrix where each entry is a random number [0, 1] # would use rand, but Generators don&#39;t have it # would use random, but old mtrand.RandomStates don&#39;t have it K = rng.uniform(size=(n_unseed, n_unseed)) # Sinkhorn balancing K = _doubly_stochastic(K) P = J * 0.5 + K * 0.5 else: P0 = np.atleast_2d(P0) _check_init_input(P0, n_unseed) P = P0 const_sum = A21 @ B21.T + A12.T @ B12 # [1] Algorithm 1 Line 2 - loop while stopping criteria not met for n_iter in range(1, maxiter + 1): # [1] Algorithm 1 Line 3 - compute the gradient of f(P) = -tr(APB^tP^t) grad_fp = const_sum + A22 @ P @ B22.T + A22.T @ P @ B22 # [1] Algorithm 1 Line 4 - get direction Q by solving Eq. 8 _, cols = linear_sum_assignment(grad_fp, maximize=maximize) Q = np.eye(n_unseed)[cols] # [1] Algorithm 1 Line 5 - compute the step size # Noting that e.g. trace(Ax) = trace(A)*x, expand and re-collect # terms as ax**2 + bx + c. c does not affect location of minimum # and can be ignored. Also, note that trace(A@B) = (A.T*B).sum(); # apply where possible for efficiency. R = P - Q b21 = ((R.T @ A21) * B21).sum() b12 = ((R.T @ A12.T) * B12.T).sum() AR22 = A22.T @ R BR22 = B22 @ R.T b22a = (AR22 * B22.T[cols]).sum() b22b = (A22 * BR22[cols]).sum() a = (AR22.T * BR22).sum() b = b21 + b12 + b22a + b22b # critical point of ax^2 + bx + c is at x = -d/(2*e) # if a * obj_func_scalar &gt; 0, it is a minimum # if minimum is not in [0, 1], only endpoints need to be considered if a * obj_func_scalar &gt; 0 and 0 &lt;= -b / (2 * a) &lt;= 1: alpha = -b / (2 * a) else: alpha = np.argmin([0, (b + a) * obj_func_scalar]) # [1] Algorithm 1 Line 6 - Update P P_i1 = alpha * P + (1 - alpha) * Q if np.linalg.norm(P - P_i1) / np.sqrt(n_unseed) &lt; tol: P = P_i1 break P = P_i1 # [1] Algorithm 1 Line 7 - end main loop # [1] Algorithm 1 Line 8 - project onto the set of permutation matrices _, col = linear_sum_assignment(-P) perm = np.concatenate((np.arange(n_seeds), col + n_seeds)) unshuffled_perm = np.zeros(n, dtype=int) unshuffled_perm[perm_A] = perm_B[perm] score = _calc_score(A, B, unshuffled_perm) res = {&quot;col_ind&quot;: unshuffled_perm, &quot;fun&quot;: score, &quot;nit&quot;: n_iter} return OptimizeResult(res) def _check_init_input(P0, n): row_sum = np.sum(P0, axis=0) col_sum = np.sum(P0, axis=1) tol = 1e-3 msg = None if P0.shape != (n, n): msg = &quot;`P0` matrix must have shape m&#39; x m&#39;, where m&#39;=n-m&quot; elif ( (~np.isclose(row_sum, 1, atol=tol)).any() or (~np.isclose(col_sum, 1, atol=tol)).any() or (P0 &lt; 0).any() ): msg = &quot;`P0` matrix must be doubly stochastic&quot; if msg is not None: raise ValueError(msg) def _split_matrix(X, n): # definitions according to Seeded Graph Matching [2]. upper, lower = X[:n], X[n:] return upper[:, :n], upper[:, n:], lower[:, :n], lower[:, n:] def _doubly_stochastic(P, tol=1e-3): # Adapted from @btaba implementation # https://github.com/btaba/sinkhorn_knopp # of Sinkhorn-Knopp algorithm # https://projecteuclid.org/euclid.pjm/1102992505 max_iter = 1000 c = 1 / P.sum(axis=0) r = 1 / (P @ c) P_eps = P for it in range(max_iter): if (np.abs(P_eps.sum(axis=1) - 1) &lt; tol).all() and ( np.abs(P_eps.sum(axis=0) - 1) &lt; tol ).all(): # All column/row sums ~= 1 within threshold break c = 1 / (r @ P) r = 1 / (P @ c) P_eps = r[:, None] * P * c return P_eps . . GM + LightSpeedLAP . #collapse import numpy as np import operator from scipy.optimize import linear_sum_assignment, OptimizeResult from scipy._lib._util import check_random_state import itertools from ot import sinkhorn def quadratic_assignment_ls(A, B, method=&quot;faq&quot;, options=None): r&quot;&quot;&quot; Approximates solution to the quadratic assignment problem and the graph matching problem. Quadratic assignment solves problems of the following form: .. math:: min_P &amp; { text{trace}(A^T P B P^T)} mbox{s.t. } &amp; {P epsilon mathcal{P}} where :math:` mathcal{P}` is the set of all permutation matrices, and :math:`A` and :math:`B` are square matrices. Graph matching tries to *maximize* the same objective function. This algorithm can be thought of as finding the alignment of the nodes of two graphs that minimizes the number of induced edge disagreements, or, in the case of weighted graphs, the sum of squared edge weight differences. Note that the quadratic assignment problem is NP-hard, is not known to be solvable in polynomial time, and is computationally intractable. Therefore, the results given are approximations, not guaranteed to be exact solutions. Parameters - A : 2d-array, square The square matrix :math:`A` in the objective function above. B : 2d-array, square The square matrix :math:`B` in the objective function above. method : str in {&#39;faq&#39;, &#39;2opt&#39;} (default: &#39;faq&#39;) The algorithm used to solve the problem. :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;` (default) and :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` are available. options : dict, optional A dictionary of solver options. All solvers support the following: partial_match : 2d-array of integers, optional, (default = None) Allows the user to fix part of the matching between the two matrices. In the literature, a partial match is also known as a &quot;seed&quot; [2]_. Each row of `partial_match` specifies the indices of a pair of corresponding nodes, that is, node ``partial_match[i, 0]`` of `A` is matched to node ``partial_match[i, 1]`` of `B`. Accordingly, ``partial_match`` is an array of size ``(m , 2)``, where ``m`` is not greater than the number of nodes. maximize : bool (default = False) Setting `maximize` to ``True`` solves the Graph Matching Problem (GMP) rather than the Quadratic Assingnment Problem (QAP). rng : {None, int, `~np.random.RandomState`, `~np.random.Generator`} This parameter defines the object to use for drawing random variates. If `rng` is ``None`` the `~np.random.RandomState` singleton is used. If `rng` is an int, a new ``RandomState`` instance is used, seeded with `rng`. If `rng` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. For method-specific options, see :func:`show_options(&#39;quadratic_assignment&#39;) &lt;show_options&gt;`. Returns - res : OptimizeResult A :class:`scipy.optimize.OptimizeResult` containing the following fields. col_ind : 1-D array An array of column indices corresponding with the best permutation of the nodes of `B` found. fun : float The corresponding value of the objective function. nit : int The number of iterations performed during optimization. Notes -- The default method :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;` uses the Fast Approximate QAP algorithm [1]_; it is typically offers the best combination of speed and accuracy. Method :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` can be computationally expensive, but may be a useful alternative, or it can be used to refine the solution returned by another method. References - .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik, S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and C.E. Priebe, &quot;Fast approximate quadratic programming for graph matching,&quot; PLOS one, vol. 10, no. 4, p. e0121002, 2015, :doi:`10.1371/journal.pone.0121002` .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski, C. Priebe, &quot;Seeded graph matching&quot;, Pattern Recognit. 87 (2019): 203-215, :doi:`10.1016/j.patcog.2018.09.014` .. [3] &quot;2-opt,&quot; Wikipedia. https://en.wikipedia.org/wiki/2-opt Examples -- &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; from scipy.optimize import quadratic_assignment &gt;&gt;&gt; A = np.array([[0, 80, 150, 170], [80, 0, 130, 100], ... [150, 130, 0, 120], [170, 100, 120, 0]]) &gt;&gt;&gt; B = np.array([[0, 5, 2, 7], [0, 0, 3, 8], ... [0, 0, 0, 3], [0, 0, 0, 0]]) &gt;&gt;&gt; res = quadratic_assignment(A, B) &gt;&gt;&gt; print(res) col_ind: array([0, 3, 2, 1]) fun: 3260 nit: 9 The see the relationship between the returned ``col_ind`` and ``fun``, use ``col_ind`` to form the best permutation matrix found, then evaluate the objective function :math:`f(P) = trace(A^T P B P^T )`. &gt;&gt;&gt; n = A.shape[0] &gt;&gt;&gt; perm = res[&#39;col_ind&#39;] &gt;&gt;&gt; P = np.eye(n, dtype=int)[perm] &gt;&gt;&gt; fun = np.trace(A.T @ P @ B @ P.T) &gt;&gt;&gt; print(fun) 3260 Alternatively, to avoid constructing the permutation matrix explicitly, directly permute the rows and columns of the distance matrix. &gt;&gt;&gt; fun = np.trace(A.T @ B[perm][:, perm]) &gt;&gt;&gt; print(fun) 3260 Although not guaranteed in general, ``quadratic_assignment`` happens to have found the globally optimal solution. &gt;&gt;&gt; from itertools import permutations &gt;&gt;&gt; perm_opt, fun_opt = None, np.inf &gt;&gt;&gt; for perm in permutations([0, 1, 2, 3]): ... perm = np.array(perm) ... fun = np.trace(A.T @ B[perm][:, perm]) ... if fun &lt; fun_opt: ... fun_opt, perm_opt = fun, perm &gt;&gt;&gt; print(np.array_equal(perm_opt, res[&#39;col_ind&#39;])) True Here is an example for which the default method, :ref:`&#39;faq&#39; &lt;optimize.qap-faq&gt;`, does not find the global optimum. &gt;&gt;&gt; A = np.array([[0, 5, 8, 6], [5, 0, 5, 1], ... [8, 5, 0, 2], [6, 1, 2, 0]]) &gt;&gt;&gt; B = np.array([[0, 1, 8, 4], [1, 0, 5, 2], ... [8, 5, 0, 5], [4, 2, 5, 0]]) &gt;&gt;&gt; res = quadratic_assignment(A, B) &gt;&gt;&gt; print(res) col_ind: array([1, 0, 3, 2]) fun: 178 nit: 13 If accuracy is important, consider using :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` to refine the solution. &gt;&gt;&gt; guess = np.array([np.arange(A.shape[0]), res.col_ind]).T &gt;&gt;&gt; res = quadratic_assignment(A, B, method=&quot;2opt&quot;, ... options = {&#39;partial_guess&#39;: guess}) &gt;&gt;&gt; print(res) col_ind: array([1, 2, 3, 0]) fun: 176 nit: 17 &quot;&quot;&quot; if options is None: options = {} method = method.lower() methods = {&quot;faq&quot;: _quadratic_assignment_faq_ls} if method not in methods: raise ValueError(f&quot;method {method} must be in {methods}.&quot;) res = methods[method](A, B, **options) return res def _calc_score(A, B, perm): # equivalent to objective function but avoids matmul return np.sum(A * B[perm][:, perm]) def _common_input_validation(A, B, partial_match): A = np.atleast_2d(A) B = np.atleast_2d(B) if partial_match is None: partial_match = np.array([[], []]).T partial_match = np.atleast_2d(partial_match).astype(int) msg = None if A.shape[0] != A.shape[1]: msg = &quot;`A` must be square&quot; elif B.shape[0] != B.shape[1]: msg = &quot;`B` must be square&quot; elif A.ndim != 2 or B.ndim != 2: msg = &quot;`A` and `B` must have exactly two dimensions&quot; elif A.shape != B.shape: msg = &quot;`A` and `B` matrices must be of equal size&quot; elif partial_match.shape[0] &gt; A.shape[0]: msg = &quot;`partial_match` can have only as many seeds as there are nodes&quot; elif partial_match.shape[1] != 2: msg = &quot;`partial_match` must have two columns&quot; elif partial_match.ndim != 2: msg = &quot;`partial_match` must have exactly two dimensions&quot; elif (partial_match &lt; 0).any(): msg = &quot;`partial_match` must contain only positive indices&quot; elif (partial_match &gt;= len(A)).any(): msg = &quot;`partial_match` entries must be less than number of nodes&quot; elif not len(set(partial_match[:, 0])) == len(partial_match[:, 0]) or not len( set(partial_match[:, 1]) ) == len(partial_match[:, 1]): msg = &quot;`partial_match` column entries must be unique&quot; if msg is not None: raise ValueError(msg) return A, B, partial_match def _quadratic_assignment_faq_ls( A, B, maximize=False, partial_match=None, rng=None, P0=&quot;barycenter&quot;, shuffle_input=False, maxiter=30, tol=0.03, ): r&quot;&quot;&quot; Solve the quadratic assignment problem (approximately). This function solves the Quadratic Assignment Problem (QAP) and the Graph Matching Problem (GMP) using the Fast Approximate QAP Algorithm (FAQ) [1]_. Quadratic assignment solves problems of the following form: .. math:: min_P &amp; { text{trace}(A^T P B P^T)} mbox{s.t. } &amp; {P epsilon mathcal{P}} where :math:` mathcal{P}` is the set of all permutation matrices, and :math:`A` and :math:`B` are square matrices. Graph matching tries to *maximize* the same objective function. This algorithm can be thought of as finding the alignment of the nodes of two graphs that minimizes the number of induced edge disagreements, or, in the case of weighted graphs, the sum of squared edge weight differences. Note that the quadratic assignment problem is NP-hard, is not known to be solvable in polynomial time, and is computationally intractable. Therefore, the results given are approximations, not guaranteed to be exact solutions. Parameters - A : 2d-array, square The square matrix :math:`A` in the objective function above. B : 2d-array, square The square matrix :math:`B` in the objective function above. method : str in {&#39;faq&#39;, &#39;2opt&#39;} (default: &#39;faq&#39;) The algorithm used to solve the problem. This is the method-specific documentation for &#39;faq&#39;. :ref:`&#39;2opt&#39; &lt;optimize.qap-2opt&gt;` is also available. Options - maximize : bool (default = False) Setting `maximize` to ``True`` solves the Graph Matching Problem (GMP) rather than the Quadratic Assingnment Problem (QAP). This is accomplished through trivial negation of the objective function. rng : {None, int, `~np.random.RandomState`, `~np.random.Generator`} This parameter defines the object to use for drawing random variates. If `rng` is ``None`` the `~np.random.RandomState` singleton is used. If `rng` is an int, a new ``RandomState`` instance is used, seeded with `rng`. If `rng` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. partial_match : 2d-array of integers, optional, (default = None) Allows the user to fix part of the matching between the two matrices. In the literature, a partial match is also known as a &quot;seed&quot;. Each row of `partial_match` specifies the indices of a pair of corresponding nodes, that is, node ``partial_match[i, 0]`` of `A` is matched to node ``partial_match[i, 1]`` of `B`. Accordingly, ``partial_match`` is an array of size ``(m , 2)``, where ``m`` is not greater than the number of nodes, :math:`n`. P0 : 2d-array, &quot;barycenter&quot;, or &quot;randomized&quot; (default = &quot;barycenter&quot;) The initial (guess) permutation matrix or search &quot;position&quot; `P0`. `P0` need not be a proper permutation matrix; however, it must be :math:`m&#39; x m&#39;`, where :math:`m&#39; = n - m`, and it must be doubly stochastic: each of its rows and columns must sum to 1. If unspecified or ``&quot;barycenter&quot;``, the non-informative &quot;flat doubly stochastic matrix&quot; :math:`J = 1*1^T/m&#39;`, where :math:`1` is a :math:`m&#39; times 1` array of ones, is used. This is the &quot;barycenter&quot; of the search space of doubly-stochastic matrices. If ``&quot;randomized&quot;``, the algorithm will start from the randomized initial search position :math:`P_0 = (J + K)/2`, where :math:`J` is the &quot;barycenter&quot; and :math:`K` is a random doubly stochastic matrix. shuffle_input : bool (default = False) To avoid artificially high or low matching due to inherent sorting of input matrices, gives users the option to shuffle the nodes. Results are then unshuffled so that the returned results correspond with the node order of inputs. Shuffling may cause the algorithm to be non-deterministic, unless a random seed is set or an `rng` option is provided. maxiter : int, positive (default = 30) Integer specifying the max number of Franke-Wolfe iterations performed. tol : float (default = 0.03) A threshold for the stopping criterion. Franke-Wolfe iteration terminates when the change in search position between iterations is sufficiently small, that is, when the relative Frobenius norm, :math:` frac{||P_{i}-P_{i+1}||_F}{ sqrt{len(P_{i})}} leq tol`, where :math:`i` is the iteration number. Returns - res : OptimizeResult A :class:`scipy.optimize.OptimizeResult` containing the following fields. col_ind : 1-D array An array of column indices corresponding with the best permutation of the nodes of `B` found. fun : float The corresponding value of the objective function. nit : int The number of Franke-Wolfe iterations performed. Notes -- The algorithm may be sensitive to the initial permutation matrix (or search &quot;position&quot;) due to the possibility of several local minima within the feasible region. A barycenter initialization is more likely to result in a better solution than a single random initialization. However, ``quadratic_assignment`` calling several times with different random initializations may result in a better optimum at the cost of longer total execution time. Examples -- As mentioned above, a barycenter initialization often results in a better solution than a single random initialization. &gt;&gt;&gt; np.random.seed(0) &gt;&gt;&gt; n = 15 &gt;&gt;&gt; A = np.random.rand(n, n) &gt;&gt;&gt; B = np.random.rand(n, n) &gt;&gt;&gt; res = quadratic_assignment(A, B) # FAQ is default method &gt;&gt;&gt; print(res.fun) 46.871483385480545 # may vary &gt;&gt;&gt; options = {&quot;P0&quot;: &quot;randomized&quot;} # use randomized initialization &gt;&gt;&gt; res = quadratic_assignment(A, B, options=options) &gt;&gt;&gt; print(res.fun) 47.224831071310625 # may vary However, consider running from several randomized initializations and keeping the best result. &gt;&gt;&gt; res = min([quadratic_assignment(A, B, options=options) ... for i in range(30)], key=lambda x: x.fun) &gt;&gt;&gt; print(res.fun) 46.671852533681516 # may vary The &#39;2-opt&#39; method can be used to further refine the results. &gt;&gt;&gt; options = {&quot;partial_guess&quot;: np.array([np.arange(n), res.col_ind]).T} &gt;&gt;&gt; res = quadratic_assignment(A, B, method=&quot;2opt&quot;, options=options) &gt;&gt;&gt; print(res.fun) 46.47160735721583 # may vary References - .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik, S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and C.E. Priebe, &quot;Fast approximate quadratic programming for graph matching,&quot; PLOS one, vol. 10, no. 4, p. e0121002, 2015, :doi:`10.1371/journal.pone.0121002` .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski, C. Priebe, &quot;Seeded graph matching&quot;, Pattern Recognit. 87 (2019): 203-215, :doi:`10.1016/j.patcog.2018.09.014` &quot;&quot;&quot; maxiter = operator.index(maxiter) # ValueError check A, B, partial_match = _common_input_validation(A, B, partial_match) msg = None if isinstance(P0, str) and P0 not in {&quot;barycenter&quot;, &quot;randomized&quot;}: msg = &quot;Invalid &#39;P0&#39; parameter string&quot; elif maxiter &lt;= 0: msg = &quot;&#39;maxiter&#39; must be a positive integer&quot; elif tol &lt;= 0: msg = &quot;&#39;tol&#39; must be a positive float&quot; if msg is not None: raise ValueError(msg) rng = check_random_state(rng) n = A.shape[0] # number of vertices in graphs n_seeds = partial_match.shape[0] # number of seeds n_unseed = n - n_seeds # check outlier cases if n == 0 or partial_match.shape[0] == n: score = _calc_score(A, B, partial_match[:, 1]) res = {&quot;col_ind&quot;: partial_match[:, 1], &quot;fun&quot;: score, &quot;nit&quot;: 0} return OptimizeResult(res) obj_func_scalar = 1 if maximize: obj_func_scalar = -1 nonseed_B = np.setdiff1d(range(n), partial_match[:, 1]) if shuffle_input: nonseed_B = rng.permutation(nonseed_B) # shuffle_input to avoid results from inputs that were already matched nonseed_A = np.setdiff1d(range(n), partial_match[:, 0]) perm_A = np.concatenate([partial_match[:, 0], nonseed_A]) perm_B = np.concatenate([partial_match[:, 1], nonseed_B]) # definitions according to Seeded Graph Matching [2]. A11, A12, A21, A22 = _split_matrix(A[perm_A][:, perm_A], n_seeds) B11, B12, B21, B22 = _split_matrix(B[perm_B][:, perm_B], n_seeds) # [1] Algorithm 1 Line 1 - choose initialization if isinstance(P0, str): # initialize J, a doubly stochastic barycenter J = np.ones((n_unseed, n_unseed)) / n_unseed if P0 == &quot;barycenter&quot;: P = J elif P0 == &quot;randomized&quot;: # generate a nxn matrix where each entry is a random number [0, 1] # would use rand, but Generators don&#39;t have it # would use random, but old mtrand.RandomStates don&#39;t have it K = rng.uniform(size=(n_unseed, n_unseed)) # Sinkhorn balancing K = _doubly_stochastic(K) P = J * 0.5 + K * 0.5 else: P0 = np.atleast_2d(P0) _check_init_input(P0, n_unseed) P = P0 const_sum = A21 @ B21.T + A12.T @ B12 # [1] Algorithm 1 Line 2 - loop while stopping criteria not met for n_iter in range(1, maxiter + 1): # [1] Algorithm 1 Line 3 - compute the gradient of f(P) = -tr(APB^tP^t) grad_fp = const_sum + A22 @ P @ B22.T + A22.T @ P @ B22 # [1] Algorithm 1 Line 4 - get direction Q by solving Eq. 8 Q = alap(grad_fp, n_unseed, maximize=maximize) # Q = np.eye(n_unseed)[cols] # [1] Algorithm 1 Line 5 - compute the step size # Noting that e.g. trace(Ax) = trace(A)*x, expand and re-collect # terms as ax**2 + bx + c. c does not affect location of minimum # and can be ignored. Also, note that trace(A@B) = (A.T*B).sum(); # apply where possible for efficiency. R = P - Q b21 = ((R.T @ A21) * B21).sum() b12 = ((R.T @ A12.T) * B12.T).sum() AR22 = A22.T @ R BR22 = B22 @ R.T b22a = (AR22 * (Q@B22.T)).sum() b22b = (A22 * (Q@BR22)).sum() a = (AR22.T * BR22).sum() b = b21 + b12 + b22a + b22b # critical point of ax^2 + bx + c is at x = -d/(2*e) # if a * obj_func_scalar &gt; 0, it is a minimum # if minimum is not in [0, 1], only endpoints need to be considered if a * obj_func_scalar &gt; 0 and 0 &lt;= -b / (2 * a) &lt;= 1: alpha = -b / (2 * a) else: alpha = np.argmin([0, (b + a) * obj_func_scalar]) # [1] Algorithm 1 Line 6 - Update P P_i1 = alpha * P + (1 - alpha) * Q if np.linalg.norm(P - P_i1) / np.sqrt(n_unseed) &lt; tol: P = P_i1 break P = P_i1 # [1] Algorithm 1 Line 7 - end main loop # [1] Algorithm 1 Line 8 - project onto the set of permutation matrices # print(P) _, col = linear_sum_assignment(-P) perm = np.concatenate((np.arange(n_seeds), col + n_seeds)) unshuffled_perm = np.zeros(n, dtype=int) unshuffled_perm[perm_A] = perm_B[perm] score = _calc_score(A, B, unshuffled_perm) res = {&quot;col_ind&quot;: unshuffled_perm, &quot;fun&quot;: score, &quot;nit&quot;: n_iter} return OptimizeResult(res) def _check_init_input(P0, n): row_sum = np.sum(P0, axis=0) col_sum = np.sum(P0, axis=1) tol = 1e-3 msg = None if P0.shape != (n, n): msg = &quot;`P0` matrix must have shape m&#39; x m&#39;, where m&#39;=n-m&quot; elif ( (~np.isclose(row_sum, 1, atol=tol)).any() or (~np.isclose(col_sum, 1, atol=tol)).any() or (P0 &lt; 0).any() ): msg = &quot;`P0` matrix must be doubly stochastic&quot; if msg is not None: raise ValueError(msg) def _split_matrix(X, n): # definitions according to Seeded Graph Matching [2]. upper, lower = X[:n], X[n:] return upper[:, :n], upper[:, n:], lower[:, :n], lower[:, n:] def alap(P, n, maximize): power = -1 if maximize else 1 # lamb = 2/np.min(P[np.nonzero(P)]) # print(lamb) lamb = 100/np.max(abs(P)) # print(np.max(abs(P))) # print(P) # P = np.exp(lamb*power*P) ones = np.ones(n) P = sinkhorn(ones, ones, P, power/lamb, stopInnerThr=5e-02) # print(P) return _doubly_stochastic(P) def _doubly_stochastic(P, tol=1e-3): # Adapted from @btaba implementation # https://github.com/btaba/sinkhorn_knopp # of Sinkhorn-Knopp algorithm # https://projecteuclid.org/euclid.pjm/1102992505 max_iter = 1000 c = 1 / P.sum(axis=0) r = 1 / (P @ c) P_eps = P for it in range(max_iter): if (np.abs(P_eps.sum(axis=1) - 1) &lt; tol).all() and ( np.abs(P_eps.sum(axis=0) - 1) &lt; tol ).all(): # All column/row sums ~= 1 within threshold break c = 1 / (r @ P) r = 1 / (P @ c) P_eps = r[:, None] * P * c return P_eps . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2021/02/25/ali-gm-ls.html",
            "relUrl": "/graph-matching/ali-s-e/2021/02/25/ali-gm-ls.html",
            "date": " • Feb 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Embedding graphs with covariates",
            "content": "Preliminaries . # collapse import os from pathlib import Path import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd import seaborn as sns from matplotlib.lines import Line2D from sklearn.metrics import accuracy_score from sklearn.model_selection import cross_val_score from sklearn.neighbors import KNeighborsClassifier from umap import UMAP from graspologic.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed, selectSVD from graspologic.plot import pairplot, screeplot from graspologic.utils import get_lcc, pass_to_ranks, to_laplace from src.io import savefig from src.visualization import adjplot, matrixplot, set_theme set_theme() FNAME = os.path.basename(__file__)[:-3] def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, **kws) . . # collapse data_dir = Path(&quot;maggot_models/data/raw/OneDrive_1_10-21-2020&quot;) covariate_loc = data_dir / &quot;product_node_embedding.csv&quot; edges_loc = data_dir / &quot;product_edges.csv&quot; category_loc = data_dir / &quot;partition_mapping.csv&quot; covariate_df = pd.read_csv(covariate_loc, index_col=0, header=None).sort_index() meta_df = pd.read_csv(category_loc, index_col=0).sort_index() assert (covariate_df.index == meta_df.index).all() edges_df = pd.read_csv(edges_loc).sort_index() g = nx.from_pandas_edgelist(edges_df, edge_attr=&quot;weight&quot;, create_using=nx.DiGraph) adj = nx.to_numpy_array(g, nodelist=meta_df.index) . . # collapse print(f&quot;Number of vertices (original): {len(adj)}&quot;) make_lcc = False if make_lcc: adj, keep_inds = get_lcc(adj, return_inds=True) print(f&quot;Number of vertices (lcc): {len(adj)}&quot;) else: # HACK need to throw out some entire classes here that have very few members y = meta_df[&quot;cat_id&quot;] unique, inv, count = np.unique(y, return_inverse=True, return_counts=True) low_count = count &lt; 5 print(f&quot;Removing categories with fewer than 5 examples: {unique[low_count]}&quot;) keep_inds = ~np.isin(inv, unique[low_count]) adj = adj[np.ix_(keep_inds, keep_inds)] print(f&quot;Number of vertices (small classes removed): {len(adj)}&quot;) meta_df = meta_df.iloc[keep_inds] covariate_df = covariate_df.iloc[keep_inds] Y = covariate_df.values . . Number of vertices (original): 1035 Removing categories with fewer than 5 examples: [3 4 6 7] Number of vertices (small classes removed): 1029 . # collapse colors = sns.color_palette(&quot;deep&quot;) palette = dict(zip(np.unique(meta_df[&quot;cat_id&quot;]), colors)) . . Adjacency matrix (sorted by category) . # collapse adjplot( pass_to_ranks(adj), plot_type=&quot;scattermap&quot;, meta=meta_df, colors=&quot;cat_id&quot;, sort_class=&quot;cat_id&quot;, palette=palette, title=r&quot;Adjacency matrix ($A$)&quot;, ) . . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee60d49150&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7fee7a5f9650&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee610c9950&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee61106410&gt;) . ax = screeplot( pass_to_ranks(adj), show_first=40, cumulative=False, title=&quot;Adjacency scree plot&quot; ) . Covariates (sorted by category) . # collapse matrixplot( Y, row_meta=meta_df, row_colors=&quot;cat_id&quot;, row_sort_class=&quot;cat_id&quot;, row_palette=palette, title=r&quot;Metadata ($X$)&quot;, ) . . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee6278fd90&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7fee627bcc90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee6278fd90&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee62783e10&gt;) . # collapse ax = screeplot(Y, show_first=40, cumulative=False, title=&quot;Covariate scree plot&quot;) . . R-LSE . # collapse lse = LaplacianSpectralEmbed(form=&quot;R-DAD&quot;) embedding = lse.fit_transform(pass_to_ranks(adj)) pairplot(embedding[0], labels=meta_df[&quot;cat_id&quot;].values, palette=palette) stashfig(&quot;pairplot-rlse&quot;) . . /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/lse.py:161: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/pairplot-rlse.png . # collapse concat_embedding = np.concatenate(embedding, axis=1) umapper = UMAP(min_dist=0.7, metric=&quot;cosine&quot;) umap_embedding = umapper.fit_transform(concat_embedding) plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{i}&quot; for i in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] fig, ax = plt.subplots(1, 1, figsize=(10, 10)) sns.scatterplot( data=plot_df, x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.7, hue=&quot;cat_id&quot;, palette=palette, ax=ax, ) ax.get_legend().remove() ax.legend(bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;) ax.set_title(&quot;UMAP o R-LSE&quot;) ax.axis(&quot;off&quot;) stashfig(&quot;umap-rlse&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/umap-rlse.png . CASE . Running CASE with a few different parameters . # collapse L = to_laplace(pass_to_ranks(adj), form=&quot;R-DAD&quot;) # D_{tau}^{-1/2} A D_{tau}^{-1/2} Y = covariate_df.values def build_case_matrix(L, Y, alpha, method=&quot;assort&quot;): if method == &quot;assort&quot;: L_case = L + alpha * Y @ Y.T elif method == &quot;nonassort&quot;: L_case = L @ L.T + alpha * Y @ Y.T elif method == &quot;cca&quot;: # doesn&#39;t make sense here, I don&#39;t thinks L_case = L @ Y return L_case def fit_case(L, Y, alpha, method=&quot;assort&quot;, n_components=None): L_case = build_case_matrix(L, Y, alpha, method=method) case_embedder = AdjacencySpectralEmbed( n_components=n_components, check_lcc=False, diag_aug=False, concat=True ) case_embedding = case_embedder.fit_transform(L_case) return case_embedding n_components = 8 alphas = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06] methods = [&quot;assort&quot;, &quot;nonassort&quot;] case_by_params = {} umap_by_params = {} for method in methods: for alpha in alphas: case_embedding = fit_case(L, Y, alpha, method=method, n_components=n_components) umapper = UMAP(min_dist=0.7, metric=&quot;cosine&quot;) umap_embedding = umapper.fit_transform(case_embedding) case_by_params[(method, alpha)] = case_embedding umap_by_params[(method, alpha)] = umap_embedding . . Plot each of the embeddings . # collapse fig, axs = plt.subplots( len(methods), len(alphas[:4]), figsize=5 * np.array([len(alphas[:4]), len(methods)]) ) for i, method in enumerate(methods): for j, alpha in enumerate(alphas[:4]): ax = axs[i, j] umap_embedding = umap_by_params[(method, alpha)] plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{c}&quot; for c in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] sns.scatterplot( data=plot_df, x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.7, hue=&quot;cat_id&quot;, palette=&quot;deep&quot;, ax=ax, ) ax.get_legend().remove() # ax.axis(&quot;off&quot;) ax.set(xticks=[], yticks=[], ylabel=&quot;&quot;, xlabel=&quot;&quot;) ax.set_title(r&quot;$ alpha$ = &quot; + f&quot;{alpha}&quot;) axs[0, -1].legend(bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;, title=&quot;Category&quot;) axs[0, 0].set_ylabel(&quot;CASE (assortative)&quot;) axs[1, 0].set_ylabel(&quot;CASE (non-assortative)&quot;) fig.suptitle(&quot;UMAP o CASE embeddings&quot;) stashfig(&quot;casc-umaps&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/casc-umaps.png . MASE . Running MASE (here I&#39;m using the Laplacian and the covariates as the two inputs) . # collapse Y = covariate_df.values n_components = 6 # TODO picked this just roughly looking at screeplots U_Y, D_Y, Vt_Y = selectSVD(Y @ Y.T, n_components=n_components) U_L, D_L, Vt_L = selectSVD(L, n_components=n_components) covariate_embedding = U_Y concatenated_latent = np.concatenate((U_L, U_Y), axis=1) U_joint, D_joint, Vt_joint = selectSVD(concatenated_latent, n_components=8) mase_embedding = U_joint . . A simple classifier on the embeddings . Here I just do a simple 5-nearest-neighbors classifier. Note that I haven&#39;t done any tuning of the classifier (like how many neighbors to use or distances other than Euclidean) or the dimension of the embedding itself. . In the experiment below, I am doing cross validation, and not treating the out-of-graph nodes any differently (i.e. they are just mixed in for the cross validation). . # collapse classifier = KNeighborsClassifier(n_neighbors=5) y = meta_df[&quot;cat_id&quot;].values rows = [] for method in methods: for alpha in alphas: X = case_by_params[(method, alpha)] cval_scores = cross_val_score(classifier, X, y=y) for score in cval_scores: rows.append({&quot;score&quot;: score, &quot;alpha&quot;: alpha, &quot;method&quot;: method}) X = mase_embedding cval_scores = cross_val_score(classifier, X, y) for score in cval_scores: rows.append({&quot;score&quot;: score, &quot;alpha&quot;: alpha + 0.01, &quot;method&quot;: &quot;MASE&quot;}) results = pd.DataFrame(rows) . . # collapse x_range = np.array(alphas) x_half_bin = 0.5 * (x_range[1] - x_range[0]) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) results[&quot;jitter_alpha&quot;] = results[&quot;alpha&quot;] + np.random.uniform( -0.0025, 0.0025, size=len(results) ) method_palette = dict(zip(np.unique(results[&quot;method&quot;]), colors)) sns.lineplot( x=&quot;alpha&quot;, hue=&quot;method&quot;, y=&quot;score&quot;, data=results, ax=ax, ci=None, palette=method_palette, ) sns.scatterplot( x=&quot;jitter_alpha&quot;, hue=&quot;method&quot;, y=&quot;score&quot;, data=results, ax=ax, palette=method_palette, s=30, ) ax.set( ylabel=&quot;Accuracy&quot;, xlabel=r&quot;$ alpha$ (CASE tuning parameter)&quot;, title=&quot;5-fold cross validation, KNN (with isolates)&quot;, ) ax.xaxis.set_major_locator(plt.FixedLocator(alphas)) ax.xaxis.set_major_formatter(plt.FixedFormatter(alphas)) ax.axvline(0.065, color=&quot;grey&quot;, alpha=0.7) mean_mase = results[results[&quot;method&quot;] == &quot;MASE&quot;][&quot;score&quot;].mean() ax.plot( [0.067, 0.073], [mean_mase, mean_mase], color=method_palette[&quot;MASE&quot;], ) handles, labels = ax.get_legend_handles_labels() handles = handles[4:] labels = labels[4:] labels[0] = &quot;Method&quot; labels[1] = r&quot;CASE$_{a}$&quot; labels[2] = r&quot;CASE$_{na}$&quot; handles.append(Line2D([0], [0], color=&quot;black&quot;)) labels.append(&quot;Mean&quot;) ax.get_legend().remove() ax.legend( bbox_to_anchor=( 1, 1, ), loc=&quot;upper left&quot;, handles=handles, labels=labels, ) for i, x in enumerate(x_range): if i % 2 == 0: ax.axvspan( x - x_half_bin, x + x_half_bin, color=&quot;lightgrey&quot;, alpha=0.3, linewidth=0, zorder=-1, ) stashfig(f&quot;knn-lcc={make_lcc}&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/knn-lcc=False.png . Using only the nodes with graph signal as training data . Here I just pick one of the parameter sets for the CASE embedding from above, as well as the MASE embedding and the embedding we get for just the covariates alone. Then I use all in-graph nodes as training data, and ask how well they predict for the out-of-graph nodes. . # collapse _, lcc_inds = get_lcc(adj, return_inds=True) not_lcc_inds = np.setdiff1d(np.arange(len(adj)), lcc_inds) y = meta_df[&quot;cat_id&quot;].values y_train = y[lcc_inds] y_test = y[not_lcc_inds] # just pick one CASE embedding method = &quot;assort&quot; alpha = 0.02 case_embedding = case_by_params[(method, alpha)] def classify_out_of_graph(X): classifier = KNeighborsClassifier(n_neighbors=5) X_train = X[lcc_inds] X_test = X[not_lcc_inds] classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) score = accuracy_score(y_test, y_pred) return score, y_pred def plot_out_of_graph(embedding, score, incorrect, method=&quot;&quot;): incorrect = y_test != y_pred umapper = UMAP( n_neighbors=10, min_dist=0.8, metric=&quot;cosine&quot;, negative_sample_rate=30 ) umap_embedding = umapper.fit_transform(embedding) plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{c}&quot; for c in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] plot_df[&quot;in_lcc&quot;] = False plot_df.loc[plot_df.index[lcc_inds], &quot;in_lcc&quot;] = True plot_df[&quot;correct&quot;] = True plot_df.loc[plot_df.index[not_lcc_inds[incorrect]], &quot;correct&quot;] = False fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.scatterplot( data=plot_df[plot_df[&quot;in_lcc&quot;]], x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.2, hue=&quot;cat_id&quot;, palette=palette, ax=ax, ) markers = dict(zip([True, False], [&quot;o&quot;, &quot;X&quot;])) sns.scatterplot( data=plot_df[~plot_df[&quot;in_lcc&quot;]], x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=30, alpha=0.9, hue=&quot;cat_id&quot;, palette=palette, style=&quot;correct&quot;, markers=markers, ax=ax, ) ax.get_legend().remove() correct_line = Line2D( [0], [0], color=&quot;black&quot;, lw=0, marker=&quot;o&quot;, mew=0, markersize=7 ) incorrect_line = Line2D( [0], [0], color=&quot;black&quot;, lw=0, marker=&quot;X&quot;, mew=0, markersize=7 ) lines = [correct_line, incorrect_line] labels = [&quot;Correct&quot;, &quot;Incorrect&quot;] ax.legend(lines, labels) ax.axis(&quot;off&quot;) ax.set_title(f&quot;{method}, predictions on isolates: accuracy {score:.2f}&quot;) return fig, ax . . score, y_pred = classify_out_of_graph(case_embedding) plot_out_of_graph(case_embedding, score, y_pred, method=&quot;CASE&quot;) stashfig(&quot;case-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/case-isolate-predictions-umap.png . score, y_pred = classify_out_of_graph(mase_embedding) plot_out_of_graph(mase_embedding, score, y_pred, method=&quot;MASE&quot;) stashfig(&quot;mase-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/mase-isolate-predictions-umap.png . score, y_pred = classify_out_of_graph(U_Y) plot_out_of_graph(U_Y, score, y_pred, method=&quot;Covariates&quot;) stashfig(&quot;covariates-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/covariates-isolate-predictions-umap.png .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/11/17/covariate-embedding.html",
            "relUrl": "/pedigo/graspologic/2020/11/17/covariate-embedding.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
  
    
        ,"post7": {
            "title": "Finding the right categorical labeling",
            "content": "Often when we have data that can be binned into discrete groups (clusters) we represent this with a label vector $y$. Usually we just make up a labeling scheme such as assigning the integers $0,...,K-1$ to represent $K$ different clusters. The $i$th element of the label vector denotes which group or cluster the $i$th sample belongs to, e.g. if the $i$th element is a $0$, then data point $i$ belongs to cluster $0$. . Many unsupervised clustering techniques seek to uncover this labeling (predict $y$). Let&#39;s imagine we ran a clustering algorithm, and it predicted the true clusters perfectly. All this means is that for each true cluster $0,...,K-1$, all of the points belonging to the same cluster in $y$ also belong in the same cluster in the predicted vector $ hat{y}$. However, if we are using an unsupervised clustering algorithm, it is unlikely that cluster $0$ in $y$ corresponds to cluster $0$ in $ hat{y}$, and so forth for all of the other clusters. There is no way for an unsupervised method to know this arbitrary labeling that we made up for the clusters, so it just made up an arbitrary labeling scheme too! . Below I present a simple algorithm that will often &quot;remap&quot; the categorical labeling in $ hat{y}$ to match $y$. It won&#39;t always work perfectly - for instance if two clusters in $ hat{y}$ both contain exactly half of a cluster from $y$, there is no way to resolve this ambiguity fairly. Still, it is often useful in practice. . import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.datasets import make_blobs from sklearn.mixture import GaussianMixture from sklearn.metrics import confusion_matrix from scipy.optimize import linear_sum_assignment sns.set_context(&quot;talk&quot;) X, y_true = make_blobs(n_samples=100, random_state=888888) n_classes = len(np.unique(y_true)) palette = dict(zip(np.arange(n_classes), sns.color_palette(&quot;deep&quot;, n_classes))) plot_df = pd.DataFrame(data=X, columns=np.arange(X.shape[1], dtype=str)) plot_df[&quot;true_labels&quot;] = y_true def simple_scatter(ax, hue, title=&quot;&quot;): sns.scatterplot(data=plot_df, x=&quot;0&quot;, y=&quot;1&quot;, hue=hue, ax=ax, palette=palette) ax.set(xticks=[], yticks=[], ylabel=&quot;&quot;, xlabel=&quot;&quot;, title=title) ax.get_legend().remove() fig, axs = plt.subplots(1, 3, figsize=(18, 6)) simple_scatter(axs[0], &quot;true_labels&quot;, title=&quot;Known labeling&quot;) gmm = GaussianMixture(n_components=3, random_state=80808) y_predicted = gmm.fit_predict(X) plot_df[&quot;predicted_labels&quot;] = y_predicted simple_scatter(axs[1], &quot;predicted_labels&quot;, title=&quot;Predicted labeling (original)&quot;) def remap_labels(y_true, y_pred, return_map=False): &quot;&quot;&quot; Remaps a categorical labeling (such as one predicted by a clustering algorithm) to match the labels used by another similar labeling. Parameters - y_true : array-like of shape (n_samples,) Ground truth labels, or, labels to map to. y_pred : array-like of shape (n_samples,) Labels to remap to match the categorical labeling of `y_true`. Returns - remapped_y_pred : np.ndarray of shape (n_samples,) Same categorical labeling as that of `y_pred`, but with the category labels permuted to best match those of `y_true`. label_map : dict Mapping from the original labels of `y_pred` to the new labels which best resemble those of `y_true`. Examples -- &gt;&gt;&gt; y_true = np.array([0,0,1,1,2,2]) &gt;&gt;&gt; y_pred = np.array([2,2,1,1,0,0]) &gt;&gt;&gt; remap_labels(y_true, y_pred) array([0, 0, 1, 1, 2, 2]) &quot;&quot;&quot; confusion_mat = confusion_matrix(y_true, y_pred) row_inds, col_inds = linear_sum_assignment(confusion_mat, maximize=True) label_map = dict(zip(col_inds, row_inds)) remapped_y_pred = np.vectorize(label_map.get)(y_pred) if return_map: return remapped_y_pred, label_map else: return remapped_y_pred y_remapped = remap_labels(y_true, y_predicted) plot_df[&quot;remapped_labels&quot;] = y_remapped simple_scatter(axs[2], &quot;remapped_labels&quot;, title=&quot;Predicted labeling (remapped)&quot;) handles, labels = axs[1].get_legend_handles_labels() _ = axs[1].legend( handles[:], labels[:], bbox_to_anchor=(0.5, 0), loc=&quot;upper center&quot;, ncol=3, title=&quot;Cluster labels&quot;, ) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/clustering/2020/10/12/remapping-labels.html",
            "relUrl": "/pedigo/graspologic/clustering/2020/10/12/remapping-labels.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Modeling the distribution of neural networks",
            "content": "# collapse import os import warnings import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from graspy.plot import heatmap, pairplot from matplotlib.transforms import blended_transform_factory from scipy.stats import pearsonr, spearmanr from sklearn.datasets import load_digits from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier from graspologic.embed import AdjacencySpectralEmbed, select_dimension from graspologic.match import GraphMatch from src.io import savefig from src.visualization import set_theme FNAME = os.path.basename(__file__)[:-3] set_theme() np.random.seed(8888) def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, print_out=False, **kws) . . Trying to learn distributions on NNs trained on the same task . Example training and predicted labels . Adapted from Sklearn docs . # collapse # The digits dataset digits = load_digits() # The data that we are interested in is made of 8x8 images of digits, let&#39;s # have a look at the first 4 images, stored in the `images` attribute of the # dataset. If we were working from image files, we could load them using # matplotlib.pyplot.imread. Note that each image must have the same size. For these # images, we know which digit they represent: it is given in the &#39;target&#39; of # the dataset. _, axes = plt.subplots(2, 4) images_and_labels = list(zip(digits.images, digits.target)) for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Training: %i&quot; % label, fontsize=&quot;x-small&quot;) # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # Create a classifier: a support vector classifier # classifier = svm.SVC(gamma=0.001) classifier = MLPClassifier() # Split data into train and test subsets X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.5, shuffle=False ) # We learn the digits on the first half of the digits classifier.fit(X_train, y_train) # Now predict the value of the digit on the second half: predicted = classifier.predict(X_test) images_and_predictions = list(zip(digits.images[n_samples // 2 :], predicted)) for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Prediction: %i&quot; % prediction, fontsize=&quot;x-small&quot;) plt.show() . . Training multiple NNs on the same task . # collapse hidden_layer_sizes = (15,) n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) n_replicates = 8 adjs = [] all_biases = [] all_weights = [] accuracies = [] mlps = [] for i in range(n_replicates): X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.3, shuffle=True ) mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=600) mlp.fit(X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) weights_by_layer = mlp.coefs_ all_biases.append(mlp.intercepts_) all_weights.append(mlp.coefs_) accuracies.append(acc) mlps.append(mlp) print(f&quot;Test accuracy score for NN {i+1}: {acc}&quot;) n_nodes = 0 for weights in weights_by_layer: n_source, n_target = weights.shape n_nodes += n_source n_nodes += n_target n_nodes += len(hidden_layer_sizes) + 1 adj = np.zeros((n_nodes, n_nodes)) n_nodes_visited = 0 for i, weights in enumerate(weights_by_layer): n_source, n_target = weights.shape adj[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] = weights adj[ -i - 1, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target ] = mlp.intercepts_[i] n_nodes_visited += n_source adjs.append(adj) all_biases = [np.concatenate(b) for b in all_biases] all_biases = np.stack(all_biases).T all_weights = [[w.ravel() for w in weights] for weights in all_weights] all_weights = [np.concatenate(w) for w in all_weights] all_weights = np.stack(all_weights).T . . Test accuracy score for NN 1: 0.95 Test accuracy score for NN 2: 0.9518518518518518 Test accuracy score for NN 3: 0.9648148148148148 Test accuracy score for NN 4: 0.9648148148148148 Test accuracy score for NN 5: 0.9462962962962963 Test accuracy score for NN 6: 0.9351851851851852 Test accuracy score for NN 7: 0.9685185185185186 Test accuracy score for NN 8: 0.9462962962962963 . Plotting the adjacency matrices for each NN . # collapse vmax = max(map(np.max, adjs)) vmin = min(map(np.min, adjs)) fig, axs = plt.subplots(2, 4, figsize=(20, 10)) for i, ax in enumerate(axs.ravel()): heatmap(adjs[i], cbar=False, vmin=vmin, vmax=vmax, ax=ax, title=f&quot;NN {i + 1}&quot;) fig.suptitle(&quot;Adjacency matrices&quot;, fontsize=&quot;large&quot;, fontweight=&quot;bold&quot;) plt.tight_layout() stashfig(&quot;multi-nn-adjs&quot;) . . Trying to model the weights with something simple . I make a new NN where the weights are set to the simple average of weights across all of the NN that I fit and see how it performs. . # collapse adj_bar = np.mean(np.stack(adjs), axis=0) def mlp_from_adjacency(adj, X_train, y_train): mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1) with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;, category=UserWarning) mlp.fit( X_train, y_train ) # dummy fit, just to set parameters like shape of input/output n_nodes_visited = 0 for i, weights in enumerate(mlp.coefs_): n_source, n_target = weights.shape mlp.coefs_[i] = adj[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] mlp.intercepts_[i] = adj[ -i - 1, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target ] n_nodes_visited += n_source return mlp X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) mlp = mlp_from_adjacency(adj_bar, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) print(f&quot;Test accuracy score for NN with mean weights: {acc}&quot;) . . Test accuracy score for NN with mean weights: 0.27106518282988873 . Why I think this doesn&#39;t work, and what we might be able to do about it . There are two big issues as I see it: . Permuation nonidentifiability in the model | Lack of edge-edge dependence structure in our models . Below I investigate the first issue, haven&#39;t thought about what to do for the second . | . Plotting the learned weights against each other . If each network has $d$ free weight parameters, and there are $T$ of them, I form the $T$ by $d$ matrix of weights per neural network, and then plot each network&#39;s weights against each other. . # collapse def corrplot(x, y, *args, ax=None, fontsize=&quot;xx-small&quot;, **kwargs): if ax is None: ax = plt.gca() pearsons, _ = pearsonr(x, y) spearmans, _ = spearmanr(x, y) text = r&quot;$ rho_p: $&quot; + f&quot;{pearsons:.3f} n&quot; text += r&quot;$ rho_s: $&quot; + f&quot;{spearmans:.3f}&quot; ax.text(1, 1, text, ha=&quot;right&quot;, va=&quot;top&quot;, transform=ax.transAxes, fontsize=fontsize) pg = pairplot( all_weights, alpha=0.1, title=&quot;Weights&quot;, col_names=[f&quot;NN {i+1}&quot; for i in range(all_weights.shape[1])], ) pg.map_offdiag(corrplot) stashfig( &quot;weight-pairplot&quot;, ) . . Can graph matching fix the permutation nonidentifiability? . Given one neural network architecture, one could permute the labels/orders of the hidden units, and the network would be functionally equivalent. This means that when comparing the architectures of two learned neural networks against each other, there is a nonidentifiability problem caused by this arbitrary permutation. Even if we imagine that two neural networks learned the exact same weights, they are unlikely to look similar at a glance because it is unlikely they learned the same weights and the same permutation. Let&#39;s see if graph matching can help resolve this nonidentifiability. . # collapse heatmap_kws = dict(vmin=vmin, vmax=vmax, cbar=False) fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 pre-GM&quot;, **heatmap_kws) heatmap(adjs[1], ax=axs[1], title=&quot;NN 2 pre-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adjs[1], ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;pre-gm-adjs&quot;) seeds = np.concatenate( ( np.arange(data.shape[1]), np.arange(len(adj) - 10 - len(hidden_layer_sizes) - 1, len(adj)), ) ) gm = GraphMatch(n_init=20, init_method=&quot;barycenter&quot;) gm.fit(adjs[0], adjs[1], seeds_A=seeds, seeds_B=seeds) perm_inds = gm.perm_inds_ adj_1_matched = adjs[1][np.ix_(perm_inds, perm_inds)].copy() fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 post-GM&quot;, **heatmap_kws) heatmap(adj_1_matched, ax=axs[1], title=&quot;NN 2 post-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adj_1_matched, ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;post-gm-adjs&quot;) fig, axs = plt.subplots(1, 2, figsize=(16, 8)) ax = axs[0] sns.scatterplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, alpha=0.3, linewidth=0, s=15) corrplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set( title=&quot;Weights pre-GM&quot;, xticks=[], yticks=[], xlabel=&quot;NN 1 weights&quot;, ylabel=&quot;NN 2 weights&quot;, ) ax.axis(&quot;equal&quot;) ax = axs[1] sns.scatterplot( adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, alpha=0.3, linewidth=0, s=15 ) corrplot(adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set( title=&quot;Weights post-GM&quot;, xticks=[], yticks=[], xlabel=&quot;NN 1 weights&quot;, ylabel=&quot;NN 2 weights&quot;, ) ax.axis(&quot;equal&quot;) stashfig(&quot;pre-post-weights-gm&quot;) . . Unraveling the nonidentifiability with GM . I match the weights in each network to the best performing one using graph matching. . NB: the way I&#39;m doing this is more convenient but probably dumb, really should just be matching on a per-hidden-layer basis. But in this case I have one hidden layer and the others are seeds so it doesn&#39;t matter. . # collapse best_model_ind = np.argmax(accuracies) best_adj = adjs[best_model_ind] matched_adjs = [] for i, adj in enumerate(adjs): gm = GraphMatch(n_init=20, init_method=&quot;barycenter&quot;) gm.fit(best_adj, adj, seeds_A=seeds, seeds_B=seeds) perm_inds = gm.perm_inds_ matched_adj = adj[np.ix_(perm_inds, perm_inds)].copy() matched_adjs.append(matched_adj) . . All pairwise weight comparisons after matching . We see that the correlation in weights improves somewhat, though they are still not highly correlated. . # collapse all_matched_weights = [a.ravel() for a in matched_adjs] all_matched_weights = np.stack(all_matched_weights, axis=1) all_matched_weights = all_matched_weights[ np.linalg.norm(all_matched_weights, axis=1) != 0, : ] all_matched_weights.shape pg = pairplot( all_matched_weights, alpha=0.1, title=&quot;Matched weights&quot;, col_names=[f&quot;NN {i+1}&quot; for i in range(all_matched_weights.shape[1])], ) pg.map_offdiag(corrplot) stashfig( &quot;matched-weight-pairplot&quot;, ) . . Using $ bar{A}_{matched}$ as the weight matrix . I take the mean of the weights after matching, and ask how well this mean weight matrix performs when converted back to a neural net. . # collapse matched_adj_bar = np.mean(np.stack(matched_adjs), axis=0) X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) mlp = mlp_from_adjacency(matched_adj_bar, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) print(f&quot;Test accuracy score for NN with average adjacency (matched): {acc}&quot;) . . Test accuracy score for NN with average adjacency (matched): 0.7090620031796503 . Decomposing the matched and unmatched adjacency matrices . # collapse def embed(A): ase = AdjacencySpectralEmbed(n_components=len(A), algorithm=&quot;full&quot;) X, Y = ase.fit_transform(A) elbow_inds, _ = select_dimension(A, n_elbows=4) elbow_inds = np.array(elbow_inds) return X, Y, ase.singular_values_, elbow_inds def screeplot(sing_vals, elbow_inds, color=None, ax=None, label=None, linestyle=&quot;-&quot;): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) plt.plot( range(1, len(sing_vals) + 1), sing_vals, color=color, label=label, linestyle=linestyle, ) plt.scatter( elbow_inds, sing_vals[elbow_inds - 1], marker=&quot;x&quot;, s=50, zorder=10, color=color, ) ax.set(ylabel=&quot;Singular value&quot;, xlabel=&quot;Index&quot;) return ax X_matched, Y_matched, sing_vals_matched, elbow_inds_matched = embed(matched_adj_bar) X_unmatched, Y_unmatched, sing_vals_unmatched, elbow_inds_unmatched = embed(adj_bar) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) screeplot(sing_vals_matched, elbow_inds_matched, ax=ax, label=&quot;matched&quot;) screeplot( sing_vals_unmatched, elbow_inds_unmatched, ax=ax, label=&quot;unmatched&quot;, linestyle=&quot;--&quot; ) ax.legend() stashfig(&quot;screeplot-adj-bars&quot;) . . Plotting accuracy as a function of adjacency rank . # collapse match_latent_map = { &quot;matched&quot;: (X_matched, Y_matched), &quot;unmatched&quot;: (X_unmatched, Y_unmatched), } n_components_range = np.unique( np.geomspace(1, len(matched_adj_bar) + 1, num=10, dtype=int) ) rows = [] n_resamples = 8 for resample in range(n_resamples): for n_components in n_components_range: X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) for method in [&quot;matched&quot;, &quot;unmatched&quot;]: X, Y = match_latent_map[method] low_rank_adj = X[:, :n_components] @ Y[:, :n_components].T mlp = mlp_from_adjacency(low_rank_adj, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) rows.append({&quot;accuracy&quot;: acc, &quot;rank&quot;: n_components, &quot;type&quot;: method}) results = pd.DataFrame(rows) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.lineplot( x=&quot;rank&quot;, y=&quot;accuracy&quot;, data=results, style=&quot;type&quot;, hue=&quot;type&quot;, ax=ax, # markers=[&quot;o&quot;, &quot;o&quot;], ) results[&quot;jitter_rank&quot;] = results[&quot;rank&quot;] + np.random.uniform( -1, 1, size=len(results[&quot;rank&quot;]) ) sns.scatterplot( x=&quot;jitter_rank&quot;, y=&quot;accuracy&quot;, data=results, hue=&quot;type&quot;, ax=ax, s=10, legend=False ) ax.set(yticks=[0.2, 0.4, 0.6, 0.8], ylim=(0, 0.82), xlabel=&quot;Rank&quot;, ylabel=&quot;Accuracy&quot;) ax.axhline(1 / 10, linestyle=&quot;:&quot;, linewidth=1.5, color=&quot;black&quot;) ax.text( 1, 1 / 10, &quot;Chance&quot;, ha=&quot;right&quot;, va=&quot;bottom&quot;, transform=blended_transform_factory(ax.transAxes, ax.transData), fontsize=&quot;small&quot;, ) stashfig(&quot;acc-by-rank&quot;) . .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "relUrl": "/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Simple random graph models in the latent space framework",
            "content": "import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import seaborn as sns from graspy.embed import AdjacencySpectralEmbed from graspy.models import DCSBMEstimator, EREstimator, SBMEstimator from graspy.models.sbm import _block_to_full, _get_block_indices from graspy.simulations import er_np, sbm rc_dict = { &quot;axes.spines.right&quot;: False, &quot;axes.spines.top&quot;: False, &quot;axes.edgecolor&quot;: &quot;grey&quot;, } for key, val in rc_dict.items(): mpl.rcParams[key] = val context = sns.plotting_context(context=&quot;talk&quot;, font_scale=1) sns.set_context(context) np.random.seed(8888) . Sample from ER model and get expected probabilities . p = 0.3 n = 200 er_graph = er_np(n, p) er_model = EREstimator(directed=False, loops=True) er_model.fit(er_graph) # hacky way of getting ER P matrix er_model.p_mat_[er_model.p_mat_ != 0] = p . Sample from SBM model and get expected probabilities . B = np.array([[0.5, 0.1], [0.1, 0.3]]) community_sizes = [n // 2, n // 2] sbm_graph, labels = sbm(community_sizes, B, return_labels=True, loops=True) # even more hacky way of getting SBM P matrix sbm_model = SBMEstimator(directed=False, loops=True) sbm_model.fit(sbm_graph, y=labels) sbm_model.block_p_ = B _, _, _block_inv = _get_block_indices(labels) sbm_model.p_mat_ = _block_to_full(B, _block_inv, sbm_graph.shape) . Sample from DCSBM model and get expected probabilities . degree_corrections = np.random.beta(2, 2, size=n) for label in np.unique(labels): mask = labels == label degree_corrections[mask] = np.sort(degree_corrections[mask])[::-1] comm_sum = np.sum(degree_corrections[mask]) degree_corrections[mask] = degree_corrections[mask] / comm_sum dcsbm_graph = sbm(community_sizes, B, dc=degree_corrections, loops=True) # super hacky way of getting DCSBM P matrix dcsbm_model = DCSBMEstimator(directed=False, loops=True) dcsbm_model.fit(dcsbm_graph, y=labels) dcsbm_model.block_p_ = B * (n // 2) ** 2 # block_p_ has a different meaning here degree_corrections = degree_corrections.reshape(-1, 1) dcsbm_model.degree_corrections_ = degree_corrections p_mat = _block_to_full(dcsbm_model.block_p_, _block_inv, dcsbm_graph.shape) p_mat = p_mat * np.outer(degree_corrections[:, 0], degree_corrections[:, -1]) dcsbm_model.p_mat_ = p_mat . graphs = [er_graph, sbm_graph, dcsbm_graph] p_mats = [er_model.p_mat_, sbm_model.p_mat_, dcsbm_model.p_mat_] model_names = [&quot;ER&quot;, &quot;SBM&quot;, &quot;DCSBM&quot;] . Embedding the expected and observed graphs with ASE . ase = AdjacencySpectralEmbed(n_components=2, check_lcc=False) # applying a rotation that just makes the plots look nicer theta = np.radians(-30) c, s = np.cos(theta), np.sin(theta) R = np.array(((c, -s), (s, c))) graph_latents = [] for graph in graphs: graph_latent = ase.fit_transform(graph) graph_latents.append(graph_latent @ R) p_mat_latents = [] for p_mat in p_mats: p_mat_latent = ase.fit_transform(p_mat) p_mat_latents.append(p_mat_latent @ R) . Plotting it all together . n_models = len(model_names) n_cols = 4 scale = 3 fig, axs = plt.subplots(n_models, n_cols, figsize=(scale * n_cols, scale * n_models)) def simple_heatmap(mat, ax): sns.heatmap( mat, ax=ax, xticklabels=False, yticklabels=False, cbar=False, cmap=&quot;RdBu_r&quot;, center=0, square=True, vmin=0, vmax=1, ) def simple_scatter(X, ax, y=None): sns.scatterplot( x=X[:, 0], y=X[:, 1], hue=y, s=15, linewidth=0.25, alpha=0.5, ax=ax, legend=False, ) ax.set( xticks=[], yticks=[], xlabel=&quot;&quot;, ylabel=&quot;&quot;, xlim=(-0.4, 1.4), ylim=(-0.4, 1.4) ) y = None for i, model_name in enumerate(model_names): graph = graphs[i] p_mat = p_mats[i] graph_latent = graph_latents[i] p_mat_latent = p_mat_latents[i] if i &gt; 0: y = labels ax = axs[i, 0] simple_heatmap(p_mat, ax) ax.set_ylabel(model_name) ax = axs[i, 1] simple_heatmap(graph, ax) ax = axs[i, 2] simple_scatter(p_mat_latent, ax, y=y) ax = axs[i, 3] simple_scatter(graph_latent, ax, y=y) axs[0, 0].set_title(r&quot;$ mathbf{P} = mathbf{X X^T}$&quot;) axs[0, 1].set_title(r&quot;$ mathbf{G} sim Bernoulli( mathbf{P})$&quot;) axs[0, 2].set_title(r&quot;$ mathbf{X}$&quot;) axs[0, 3].set_title(r&quot;$ mathbf{ hat{X}}$&quot;) . Text(0.5, 1.0, &#39;$ mathbf{ hat{X}}$&#39;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "relUrl": "/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
  
    
        ,"post11": {
            "title": "Shuffling Alters QAP Results",
            "content": "%pylab inline import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . Populating the interactive namespace from numpy and matplotlib . from graspy.simulations import er_corr rho = 0.9 p = 0.5 n = 20 G1, G2 = er_corr(n, p, rho, directed = False, loops = False) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;shuffle_input&#39;: False}) scores[i] =res.fun . scores . array([168., 168., 168., 168., 168.]) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;rng&#39;: i}) scores[i] =res.fun . scores . array([174., 162., 164., 168., 164.]) .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Can you make the objective function better via gm, compared to the purported 1-1?",
            "content": "# collapse import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . . # collapse %pylab inline import pandas as pd from graspy.utils import pass_to_ranks import seaborn as sns . . Populating the interactive namespace from numpy and matplotlib . Experiment Summary . If $A_i$ is the adjacency matrix at time index $i$, then with $n$ time indices, for $i = [1, n-1]$ do $GM(A_i, A_{i+1})$, where $A_i$ and $A_{i+1}$ are pre-matched based on the known 1-1 correspondance. . For each graph pair, run $GM$ $t = 20$ times, with each $t$ corresponding to a different random permutation on $A_{i+1}$. . Internally in GM, $A_{i+1}$ is shuffled, that is $A_{i+1}&#39; = Q A_{i+1} Q^T,$ where $Q$ is sampled uniformly from the set of $m x m$ permutations matrices, where $m$ is the size of the vertex set. . $GM$ is run from the barycenter ($ gamma = 0$). . Compare the objective function values of the matrices with the known matching ($trace (A_i A_{i+1}^T)$) and the average objective function resulting from $t$ runs of $GM(A_i, A_{i+1})$ . # collapse def load_adj(file): df = pd.read_csv(f&#39;org_sig1_max/{file}.csv&#39;, names = [&#39;from&#39;, &#39;to&#39;, &#39;weight&#39;]) return df . . # collapse times = [1,4,11,17,25,34,45,48,52,55,63,69,70,76,80,83,90,97,103,111,117,129,130,132,139,140,146,153,160,167, 174,181,188,192,195,202,209,216,223,229] . . # collapse from scipy.stats import sem t = 20 ofvs = np.zeros((len(times)-1,3)) # [opt_ofv, gm_ofv] for i in range(len(times)-1): # constructing the adjacency matrices Ael = load_adj(times[i]) Bel = load_adj(times[i+1]) nodes = np.concatenate((Ael[&#39;from&#39;],Ael[&#39;to&#39;],Bel[&#39;from&#39;],Bel[&#39;to&#39;]), axis=0) nodes = list(set(nodes)) n = len(nodes) A = np.zeros((n,n)) B = np.zeros((n,n)) row_list_A = [nodes.index(x) for x in Ael[&#39;from&#39;]] col_list_A = [nodes.index(x) for x in Ael[&#39;to&#39;]] A[row_list_A, col_list_A] = Ael[&#39;weight&#39;] row_list_B = [nodes.index(x) for x in Bel[&#39;from&#39;]] col_list_B = [nodes.index(x) for x in Bel[&#39;to&#39;]] B[row_list_B, col_list_B] = Bel[&#39;weight&#39;] A = pass_to_ranks(A) B = pass_to_ranks(B) gm_ofvs = np.zeros(t) for j in range(t): gmp = {&#39;maximize&#39;:True} res = quadratic_assignment(A,B, options=gmp) gm_ofvs[j] = res.fun gm_ofv = np.mean(gm_ofvs) gm_error = sem(gm_ofvs) opt_ofv = (A*B).sum() ofvs[i,:] = [opt_ofv, gm_ofv, 2*gm_error] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.scatter(np.arange(len(times)-1), ofvs[:,0], label = &#39;opt ofv&#39;) #plt.scatter(np.arange(len(times)), ofvs[:,1], label = &#39;gm ofv&#39;) plt.errorbar(np.arange(len(times)-1),ofvs[:,1], ofvs[:,2],label = &#39;average gm ofv +/- 2 s.e.&#39;,marker=&#39;o&#39;, fmt = &#39; &#39; ,capsize=3, elinewidth=1, markeredgewidth=1,color=&#39;orange&#39;) plt.legend() plt.ylabel(&#39;objective function value&#39;) plt.xlabel(&#39;time stamp (A_x &amp; A_{x+1})&#39;) . . Text(0.5, 0, &#39;time stamp (A_x &amp; A_{x+1})&#39;) . Extremely low variance above (error bars not visible) . # collapse plt.scatter(np.arange(len(times)-1), ofvs[:,1]/ofvs[:,0]) plt.hlines(1,0,40,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;y=1 (above means gm maximes ofv more)&#39;) plt.legend() plt.xlabel(&#39;Time Stamp&#39;) plt.ylabel(&#39;gm_ofv / pre-matched_ofv&#39;) . . Text(0, 0.5, &#39;gm_ofv / pre-matched_ofv&#39;) . # collapse df = pd.DataFrame(ofvs,columns=[&quot;Pre Matched OFV&quot;,&quot;Avergae GM OFV&quot;, &quot;2*s.e. GM OFV&quot;]) print(df) . . Pre Matched OFV Avergae GM OFV 2*s.e. GM OFV 0 77.778503 83.203695 6.520387e-15 1 102.363435 105.635371 6.520387e-15 2 539.611816 586.160090 5.216310e-14 3 184.831288 198.412628 2.608155e-14 4 159.324311 203.971815 0.000000e+00 5 984.399156 1152.762630 1.043262e-13 6 1089.477143 1119.627925 0.000000e+00 7 1259.423017 1290.748354 2.086524e-13 8 1585.077755 1730.198821 0.000000e+00 9 1842.297675 2111.213946 2.086524e-13 10 2144.029010 2320.060471 2.086524e-13 11 2031.803286 2092.043201 2.086524e-13 12 1956.629442 2111.907016 2.086524e-13 13 2449.635493 2580.498796 0.000000e+00 14 2488.814694 2536.434492 2.086524e-13 15 2435.361725 2648.848434 2.086524e-13 16 2450.644416 2909.986251 0.000000e+00 17 2799.810641 3086.113044 4.173048e-13 18 2795.063975 3051.412096 2.086524e-13 19 3093.261915 3183.004837 4.173048e-13 20 3483.603820 3653.248466 0.000000e+00 21 3788.502289 3834.393669 2.086524e-13 22 3837.060440 3977.016536 4.173048e-13 23 3625.742003 3800.326356 0.000000e+00 24 3067.810931 3308.850782 2.086524e-13 25 3440.573652 3634.710350 0.000000e+00 26 4605.778803 4669.727638 4.173048e-13 27 4541.053424 4634.822284 4.173048e-13 28 4305.613442 4587.914977 4.173048e-13 29 4103.909802 4283.742180 4.173048e-13 30 3897.553992 4100.491185 4.173048e-13 31 3744.066025 3871.777437 2.086524e-13 32 3206.210282 3391.022125 0.000000e+00 33 3073.053348 3358.433002 4.173048e-13 34 3177.553500 3395.320322 0.000000e+00 35 3332.128518 3368.928739 2.086524e-13 36 3180.781410 3517.218062 0.000000e+00 37 3385.105045 3510.301189 0.000000e+00 38 3229.233901 3407.950538 2.086524e-13 .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "GM+SS using JAgt Seedless Procrustes",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 75$ and $ vec{n}=[n_1,n_2,n_3] = [25,25,25]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$, $GM_{SS}$ &amp; $GM_{J.Agt}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ &amp; $GM_{J.Agt}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Description of $GM_{J.Agt}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . With $2^d$ initializations, where $d$ is dimension, use J.Agt&#39;s seedless procrustes to find the optimal orthogonal alignment matrix, $Q$. . let $Phat = hat{X}_1 Q hat{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . # collapse # load in J.Agt data # ratios_j = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) # ratios_ss_j = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_j = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) # scores_ss_j = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) # rhos = np.arange(5,10.5,0.5) *0.1 # n_p = len(rhos) # ratios_opt_j = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) # ratios_opt_ss_j = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_opt_j = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) # scores_opt_ss_j = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios_j[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios_j[i,:] ) for i in range(n_p)]) error_j = np.asarray([sem(ratios_ss_j[i,:]) for i in range(n_p)]) average_j = np.asarray([np.mean(ratios_ss_j[i,:] ) for i in range(n_p)]) . . # collapse error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) error2 = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average2 = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) # plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) # plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_j, error_j,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+J.Agt&#39;, color=&#39;green&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7f8588c091c0&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from .jagt import SeedlessProcrustes from graspy.embed import AdjacencySpectralEmbed def run_sim(r, t, n=150, flip=&#39;median&#39;): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n def _median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val1 = np.sign(X1_medians).astype(int) X1 = np.multiply(val1.reshape(-1, 1).T, X1) val2 = np.sign(X2_medians).astype(int) X2 = np.multiply(val2.reshape(-1, 1).T, X2) return X1, X2 #rhos = 0.1 * np.arange(11)[5:] m = r rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None ase = AdjacencySpectralEmbed(n_components=3, algorithm=&#39;truncated&#39;) Xhat1 = ase.fit_transform(A1) Xhat2 = ase.fit_transform(A2) if flip==&#39;median&#39;: xhh1, xhh2 = _median_sign_flips(Xhat1, Xhat2) S = xhh1 @ xhh2.T elif flip==&#39;jagt&#39;: sp = SeedlessProcrustes().fit(Xhat1, Xhat2) xhh1 = Xhat1@sp.Q xhh2 = Xhat2 S = xhh1 @ xhh2.T else: S = None for j in range(t): res = quadratic_assignment_sim(A1, A2, True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "date": " • Aug 24, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Recreating Youngser's R code figures",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . All values were set to best replicate Youngser&#39;s Figure . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 odds = [i for i in range(len(rhos)) if i%2==0] n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) . . Ali&#39;s Figure (with Python) . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f8c33d30&gt; . Youngser&#39;s Figure (with R) . . Eyeballing both onto the same figure . # collapse gm_ss_y = [0.01, 0.03, 0.12, 0.42, 0.77, 1.0] gm_y = [0, 0, 0.03, 0.15, 0.62, 1.0] sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.plot(rhos[odds],gm_ss_y, color=&#39;blue&#39;, marker=&#39;^&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.plot(rhos[odds],gm_y, color=&#39;red&#39;, marker=&#39;^&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;triangles = youngser, circle = ali&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5ef18bbe0&gt; . # collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix. . As we see from the figures below, for vanilla GM, the best case seems to consistently perform better, but with GM+SS they appear to be consistently about the same . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . # collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def run_sim(r, t): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = r #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 20)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f6a5fd90&gt; . #collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . #collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 30)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 30. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f5ed1430&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Graph matching with spectral similarity",
            "content": "#collapse from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from graspy.embed import AdjacencySpectralEmbed . . #collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed import seaborn as sns . . #collapse from qap_sim import quadratic_assignment_sim . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter. . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=20$ . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . #collapse def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 1 t = 10 rhos = 0.1 * np.arange(11) ratios2 = np.zeros((11,m)) scores2 = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False . . #collapse n = 150 m = 50 t = 20 rhos = 0.1 * np.arange(11) ratios = np.zeros((11,m)) scores = np.zeros((11,m)) ratios_ss = np.zeros((11,m)) scores_ss = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False #np.random.seed(8888) for k, rho in enumerate(rhos): for i in range(m): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): seed = k+m+t res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratios[k,i] = match_ratio(res_opt[&#39;col_ind&#39;], n) scores[k,i] = res_opt[&#39;score&#39;] ratios_ss[k,i] = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) scores_ss[k,i] = res_opt_ss[&#39;score&#39;] #ratios[k] = ratios[k]/m . . #collapse from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(11)] average = [np.mean(ratios[i,:] ) for i in range(11)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(11)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(11)] . . #collapse sns.set_context(&#39;talk&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&quot;png&quot;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . . &lt;ipython-input-111-9d9e37bc5d45&gt;:8: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument &#34;fmt&#34; which is no longer supported as of 3.3 and will become an error two minor releases later plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&#34;png&#34;, dpi=150, facecolor=&#34;w&#34;, bbox_inches=&#34;tight&#34;, pad_inches=0.3) . #collapse diff = ratios_ss[9,:] - ratios[9,:] plt.hist(diff, bins=20) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (GM+SS - GM)&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse left_adj = np.genfromtxt(&#39;left_adj.csv&#39;, delimiter=&#39;,&#39;) right_adj = np.genfromtxt(&#39;right_adj.csv&#39;, delimiter=&#39;,&#39;) . . #collapse def median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val = np.multiply(X1_medians, X2_medians) t = (val &gt; 0) * 2 - 1 X1 = np.multiply(t.reshape(-1, 1).T, X1) return X1, X2 . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Comparing multiple graph samples over time using latent distributions",
            "content": "import time import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from graspy.inference import LatentDistributionTest from graspy.simulations import p_from_latent, sample_edges from graspy.utils import symmetrize from hyppo.discrim import DiscrimOneSample from sklearn.metrics import pairwise_distances np.random.seed(8888) sns.set_context(&quot;talk&quot;) mpl.rcParams[&quot;axes.edgecolor&quot;] = &quot;lightgrey&quot; mpl.rcParams[&quot;axes.spines.right&quot;] = False mpl.rcParams[&quot;axes.spines.top&quot;] = False def hardy_weinberg(theta): &quot;&quot;&quot; Maps a value from [0, 1] to the hardy weinberg curve. &quot;&quot;&quot; hw = [theta ** 2, 2 * theta * (1 - theta), (1 - theta) ** 2] return np.array(hw).T def sample_hw_graph(thetas): latent = hardy_weinberg(thetas) p_mat = p_from_latent(latent, rescale=False, loops=False) graph = sample_edges(p_mat, directed=False, loops=False) return (graph, p_mat, latent) . Parameters of the experiment . n_timepoints = 5 n_verts = 100 n_graphs_per_timepoint = 10 deltas = np.linspace(0, 2, n_timepoints) . Distributions in latent space . Let $HW( theta)$ be the Hardy-Weinberg distribution in $ mathbb{R}^3$. . Latent positions are distributed along this curve: $$X sim HW( theta)$$ With the distribution along the curve following a Beta distribution: $$ theta sim Beta(1, 1 + delta)$$ Let $ delta$ be a proxy for &quot;time&quot; . Below I plot the distributions of $ theta$ for each value of $ delta$, where we will use a different value of $ delta$ for each time point. . fig, ax = plt.subplots(1, 1, figsize=(8, 4)) for delta in deltas: thetas = np.random.beta( 1, 1 + delta, 10000 ) # fake # to make the distributions look cleaner sns.distplot(thetas, label=delta, ax=ax) plt.legend(title=r&quot;$ delta$&quot;, bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;) _ = ax.set(ylabel=&quot;Frequency&quot;, yticks=[], xlabel=r&quot;$ theta$&quot;) . Sample latent positions, and then sample graphs . To generate each graph I sample a set of latent positions from the Hardy-Weinberg curve described above. Each time point will have multiple sets of latent positions sampled i.i.d. from the same distribution in latent space, then a single graph is sampled from each set of latent positions. . graphs = [] latents = [] times = [] for t, delta in enumerate(deltas): for i in range(n_graphs_per_timepoint): thetas = np.random.beta(1, 1 + delta, n_verts) graph, pmat, latent = sample_hw_graph(thetas) graphs.append(graph) times.append(t) latents.append(latent) times = np.array(times) . Plot 2 example sets of sampled latent positions for each time point . Here I just show the first two dimensions of true latent positions. From each of these we sample a graph. . fig, axs = plt.subplots( 2, n_timepoints, figsize=(n_timepoints * 4, 8), sharex=True, sharey=False, # TODO fix sharey and labeling ) for t, delta in enumerate(deltas): for i in range(2): ax = axs[i, t] latent = latents[t * n_graphs_per_timepoint + i] plot_latent = pd.DataFrame(latent) sns.scatterplot(data=plot_latent, x=0, y=1, ax=ax, linewidth=0, alpha=0.5, s=20) ax.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, xticks=[], yticks=[]) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Plot adjacency matrices for 2 graphs from each time point . fig, axs = plt.subplots(2, n_timepoints, figsize=(n_timepoints * 4, 8)) for t, delta in enumerate(deltas): for i in range(2): graph = graphs[t * n_graphs_per_timepoint + i] ax = axs[i, t] sns.heatmap( graph, ax=ax, cbar=False, xticklabels=False, yticklabels=False, cmap=&quot;RdBu_r&quot;, square=True, center=0, ) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Compute the test statistics for Latent Distribution Test (nonpar). . curr_time = time.time() pval_mat = np.zeros((len(graphs), len(graphs))) tstat_mat = np.zeros((len(graphs), len(graphs))) n_comparisons = (len(graphs) * (len(graphs) - 1)) / 2 counter = 0 for i, graph1 in enumerate(graphs): for j, graph2 in enumerate(graphs): if i &lt; j: ldt = LatentDistributionTest(n_bootstraps=200, workers=1) ldt.fit(graph1, graph2) pval_mat[i, j] = ldt.p_value_ tstat_mat[i, j] = ldt.sample_T_statistic_ pval_mat = symmetrize( pval_mat, method=&quot;triu&quot; ) # need to do way more bootstraps to be meaningful tstat_mat = symmetrize(tstat_mat, method=&quot;triu&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . 5.981 minutes elapsed . All pairwise test statistics and p-values . Here I show test statistics for the latent position test between all possible pairs of graphs. Higher means more different. The test statistic being used here is the 2-sample dcorr test statistic on the estimated latent positions. Note that I&#39;m not doing the new seedless alignment here (but I&#39;d like to). . Then, I show the same for the p-values. . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( tstat_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test statistics&quot;, ) fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( pval_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) _ = ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test p-values&quot;, ) . Computing discriminability . Looks at whether distances between samples from the same object (time point, in this case) are smaller than distances between samples from different objects. In a sense, it&#39;s looking at whether the diagonal blocks in the above are smaller than the rest of the matrix. Here I&#39;m using the test statistic from above as the distance. Permutation test is used to test whether one&#39;s ability to discriminate between &quot;multiple samples&quot; from the same object is highter than one would expect by chance. . curr_time = time.time() discrim = DiscrimOneSample(is_dist=True) discrim.test(tstat_mat, times) print(f&quot;Discriminability one-sample p-value: {discrim.pvalue_}&quot;) print(f&quot;Discriminability test statistic: {discrim.stat}&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . Discriminability one-sample p-value: 0.001 Discriminability test statistic: 0.8648333333333332 0.061 minutes elapsed . Test statistics and p-values as a function of time difference . Here I just play with plotting these test statistics and p-values as a function of how different in time the two graphs were. I add jitter to the time difference values just for visibility. . time_dist_mat = pairwise_distances(times.reshape((-1, 1)), metric=&quot;manhattan&quot;) triu_inds = np.triu_indices_from(time_dist_mat, k=1) time_dists = time_dist_mat[triu_inds] + np.random.uniform(-0.2, 0.2, len(triu_inds[0])) latent_dists = tstat_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=latent_dists, s=10, linewidth=0, alpha=0.3, ax=ax) ax.set(ylabel=&quot;Test statistic&quot;, xlabel=&quot;Difference in time&quot;) pval_dists = pval_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=pval_dists, s=10, linewidth=0, alpha=0.3, ax=ax) _ = ax.set(ylabel=&quot;p-value&quot;, xlabel=&quot;Difference in time&quot;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "relUrl": "/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://neurodata.github.io/notebooks/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://neurodata.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://neurodata.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://neurodata.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}